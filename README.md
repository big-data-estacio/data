# Painel Streamlit para visualizar, analisar e prever dados de vendas de um restaurante


<p align="center">
 <img src="client/src/public/streamlit.png" width="350" />
</p>


# Projeto Pedacinho do C√©u

![Python](https://img.shields.io/badge/python-^3.8-blue.svg?longCache=true&style=flat-square&colorA=4c566a&colorB=5e81ac&logo=Python&logoColor=white)
![Tableau Server Client](https://img.shields.io/badge/tableauserverclient-0.25-blue.svg?longCache=true&style=flat-square&colorA=4c566a&colorB=5e81ac&logo=ChatBot&logoColor=white)
![Requests](https://img.shields.io/badge/Requests-^v2.28.1-red.svg?longCache=true&style=flat-square&colorA=4c566a&colorB=5e81ac&logo=Python&logoColor=white)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-^2.0.0-red.svg?longCache=true&style=flat-square&logo=scala&logoColor=white&colorA=4c566a&colorB=bf616a)
![GitHub Last Commit](https://img.shields.io/github/last-commit/google/skia.svg?style=flat-square&colorA=4c566a&logo=GitHub&colorB=a3be8c)
[![GitHub Issues](https://img.shields.io/github/issues/toddbirchard/tableau-extraction.svg?style=flat-square&colorA=4c566a&logo=GitHub&colorB=ebcb8b)](https://github.com/big-data-estacio/data/issues)
[![GitHub Stars](https://img.shields.io/github/stars/toddbirchard/tableau-extraction.svg?style=flat-square&colorA=4c566a&logo=GitHub&colorB=ebcb8b)](https://github.com/big-data-estacio/data)
[![GitHub Forks](https://img.shields.io/github/forks/toddbirchard/tableau-extraction.svg?style=flat-square&colorA=4c566a&logo=GitHub&colorB=ebcb8b)](https://github.com/big-data-estacio/data)


Este painel foi desenvolvido usando Streamlit. V√°rios pacotes de c√≥digo aberto s√£o usados para processar os dados e gerar as visualiza√ß√µes, por exemplo. [pandas](https://pandas.pydata.org/), [geopandas](https://geopandas.org), [leafmap](https://leafmap.org), [matplotlib](https://matplotlib.org/) e [pydeck](https://deckgl.readthedocs.io).

Este √© um projeto de an√°lise de dados para um bar e restaurante localizado no sul da ilha de Florian√≥polis. O objetivo do projeto √© utilizar as informa√ß√µes dispon√≠veis para realizar an√°lises e obter insights que possam ser utilizados para tomar decis√µes estrat√©gicas.

> **Nota**: Resultados de gr√°ficos e tabelas no power bi, est√£o dispon√≠veis na pasta `data/processed`.

<!-- powerBi.jpeg -->
![Power BI](client/src/public/powerBi.jpeg)

## √çndice

1. [üöÄIntrodu√ß√£o](#introdu√ß√£o)
2. [üîÆArvore de Diretorios](#arvore-de-diretorios)
3. [üåÉArquitetura do projeto](#arquitetura-do-projeto)
4. [üéÜTecnologias Utilizadas](#tecnologias-utilizadas)
5. [üíéPr√©-requisitos](#pr√©-requisitos)
6. [‚ú®Instalando o Projeto](#instalando-o-projeto)
7. [üéâExecutando o Projeto](#executando-o-projeto)
   * [Configurando o ambiente virtual](#configurando-o-ambiente-virtual)
   * [Continuando a instala√ß√£o](#continuando-a-instala√ß√£o)
8. [üëæTestes](#testes)
9. [üî•Utilizando a API com Insomnia](#utilizando-a-api-com-insomnia)
10. [ü•∂Vers√£o atual](#vers√£o-atual)
11. [üëπColetando Dados](#coletando-dados)
12. [üëªProcessando Dados](#processando-dados)
13. [ü§ñVisualizando os Dados](#visualizando-os-dados)
14. [üëΩFuturas Atualiza√ß√µes](#futuras-atualiza√ß√µes)
15. [üê≥Tecnologias e conceitos utilizados](#tecnologias-e-conceitos-utilizados)
    * [Hadoop](#hadoop)
    * [Kafka](#kafka)
    * [Docker](#docker)
    * [SOLID](#solid)
    * [Padr√µes de commit](#padr√µes-de-commit)
16. [üß†Roadmap v1](#roadmap-v1)
    * [Travis CI](#travis-ci)
    * [Princ√≠pios SOLID](#princ√≠pios-solid)
    * [SQLAlchemy com PostgreSQL ou MySQL](#sqlalchemy-com-postgresql-ou-mysql)
    * [Autentica√ß√£o e autoriza√ß√£o](#autentica√ß√£o-e-autoriza√ß√£o)
    * [Interface de usu√°rio](#interface-de-usu√°rio)
17. [üß†Roadmap v2](#roadmap-v2)
    * [Roadmap v1.1](#roadmap-v1.1)
    * [Roadmap v1.2](#roadmap-v1.2)
    * [Roadmap v1.3](#roadmap-v1.3)
    * [Roadmap v2.0](#roadmap-v2.0)
18. [ü§ñO que √© o Apache Spark?](#o-que-√©-o-apache-spark?)
29. [üéñÔ∏èCrit√©rios de aceita√ß√£o do projeto](#crit√©rios-de-aceita√ß√£o-do-projeto)
20. [üëπContribuindo](#contribuindo)
21. [üëæContribuidores](#contribuidores)
22. [üéâLicen√ßa](#licen√ßa)


## Introdu√ß√£o

Este projeto √© um estudo de caso de Big Data e Power BI. O objetivo √© demonstrar como coletar, processar, analisar e visualizar grandes volumes de dados em tempo real usando ferramentas e tecnologias como Python, SQLite e Power BI.

O projeto √© baseado em um restaurante chamado "Pedacinho do C√©u". O restaurante est√° localizado em uma cidade tur√≠stica e serve comida tradicional da regi√£o. O restaurante est√° interessado em coletar dados de v√°rias fontes para analisar e obter insights sobre o neg√≥cio. O restaurante tamb√©m deseja criar visualiza√ß√µes e relat√≥rios interativos para ajudar na tomada de decis√µes e na compreens√£o de tend√™ncias e padr√µes nos dados.

O mesmo utiliza a linguagem Python e diversas bibliotecas para an√°lise de dados, como Pandas, NumPy e Plotly. Al√©m disso, √© utilizado o framework Streamlit para a cria√ß√£o de uma interface interativa para o usu√°rio, permitindo a visualiza√ß√£o dos dados e a intera√ß√£o com as funcionalidades desenvolvidas.

Entre as funcionalidades desenvolvidas, est√£o a an√°lise de vendas por m√™s, a previs√£o de clientes para o pr√≥ximo m√™s, a an√°lise de dados de clientes cadastrados, a exibi√ß√£o de um mapa de localiza√ß√£o do estabelecimento e a cria√ß√£o de uma hist√≥ria do bar e restaurante.

Para utilizar o projeto, basta clonar o reposit√≥rio e instalar as depend√™ncias listadas no arquivo requirements.txt. Em seguida, execute o comando "streamlit run app.py" para iniciar a aplica√ß√£o.


1. Coleta de dados: O projeto utiliza diversas fontes de dados, como arquivos CSV, APIs e bancos de dados. Os dados s√£o coletados e processados em uma variedade de formatos para atender √†s necessidades do projeto. A coleta de dados √© um processo crucial para garantir que as informa√ß√µes corretas estejam dispon√≠veis para an√°lise e visualiza√ß√£o.

2. Processamento de dados: O projeto utiliza uma variedade de t√©cnicas para processar e limpar os dados coletados, incluindo o uso de bibliotecas Python para an√°lise e transforma√ß√£o de dados. Os dados s√£o organizados e limpos para garantir que estejam prontos para an√°lise e visualiza√ß√£o. O processamento de dados √© uma etapa importante no processo de garantir que as informa√ß√µes corretas estejam dispon√≠veis para o usu√°rio final.

3. Armazenamento de dados: O projeto utiliza uma variedade de tecnologias de armazenamento de dados, incluindo bancos de dados relacionais e n√£o relacionais, armazenamento em nuvem e arquivos CSV. Os dados s√£o armazenados de forma a garantir que estejam seguros e dispon√≠veis para an√°lise e visualiza√ß√£o. O armazenamento de dados √© um componente cr√≠tico do projeto, garantindo que os dados estejam dispon√≠veis quando necess√°rios.

4. An√°lise e visualiza√ß√£o de dados: O projeto utiliza ferramentas de an√°lise e visualiza√ß√£o de dados, como Power BI e bibliotecas Python, para extrair informa√ß√µes significativas dos dados coletados. As informa√ß√µes s√£o apresentadas de forma clara e intuitiva, permitindo que o usu√°rio final compreenda facilmente os insights obtidos a partir dos dados. A an√°lise e visualiza√ß√£o de dados s√£o etapas cr√≠ticas no processo de transforma√ß√£o de dados em informa√ß√µes √∫teis.

5. Atualiza√ß√£o e manuten√ß√£o: O projeto requer atualiza√ß√µes regulares para garantir que os dados estejam atualizados e precisos. Al√©m disso, a manuten√ß√£o do sistema √© importante para garantir que as tecnologias utilizadas estejam atualizadas e seguras. A atualiza√ß√£o e manuten√ß√£o do sistema s√£o etapas cr√≠ticas para garantir que o projeto continue funcionando de forma eficiente e eficaz ao longo do tempo.

O projeto utiliza conceitos de Big Data e Power BI. Vamos revisar como cada tecnologia √© aplicada no projeto:

1. Linguagem de programa√ß√£o Python para desenvolvimento do back-end e front-end da aplica√ß√£o web, utilizando o framework Streamlit para cria√ß√£o da interface do usu√°rio.
2. Pandas e Numpy para manipula√ß√£o e an√°lise de dados, realizando opera√ß√µes como sele√ß√£o, filtragem, agrupamento e jun√ß√£o de dados.
3. Matplotlib e Plotly para cria√ß√£o de gr√°ficos e visualiza√ß√µes de dados interativas.
4. Scikit-Learn para modelagem de dados, com algoritmos de aprendizado de m√°quina para previs√£o e classifica√ß√£o.
5. Power BI para cria√ß√£o de dashboards e relat√≥rios interativos e visuais a partir dos dados gerados e analisados na aplica√ß√£o web.

Combinando Big Data e Power BI, este projeto oferece uma solu√ß√£o completa para coletar, processar, analisar e visualizar grandes volumes de dados em tempo real, ajudando na tomada de decis√µes informadas e oferecendo insights valiosos para o neg√≥cio do restaurante "Pedacinho do C√©u".



## Arvore de Diretorios

Abaixo est√° a estrutura de diret√≥rios do projeto:

```bash
.
‚îú‚îÄ‚îÄ üìÇ .github
‚îú‚îÄ‚îÄ üìÇ .husky
‚îú‚îÄ‚îÄ üìÇ .vscode
‚îú‚îÄ‚îÄ üìÇ assets
‚îú‚îÄ‚îÄ üìÇ backend
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ admin
‚îÇ       ‚îú‚îÄ‚îÄ üìÇ conf
‚îÇ       ‚îú‚îÄ‚îÄ üìÇ data_crawlers
‚îÇ       ‚îú‚îÄ‚îÄ üìÇ target_url_crawlers
‚îÇ       ‚îú‚îÄ‚îÄ .gitignore
‚îÇ       ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îú‚îÄ‚îÄ go_spider.py
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ üìÇ build
‚îú‚îÄ‚îÄ üìÇ client
‚îú‚îÄ‚îÄ üìÇ docs
‚îú‚îÄ‚îÄ üìÇ myenv
‚îú‚îÄ‚îÄ üìÇ docs
‚îú‚îÄ‚îÄ üìÇ src
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ api
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ data
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ error
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ log
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ public
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ scripts
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .editorconfig
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .npmrc
‚îú‚îÄ‚îÄ .travis.yml
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ architeture.md
‚îú‚îÄ‚îÄ AUTHORS.md
‚îú‚îÄ‚îÄ CHANGELOG.md
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ Procfile
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ runtime.txt
‚îú‚îÄ‚îÄ SECURITY.md
‚îî‚îÄ‚îÄ setup.sh
```


## Arquitetura do projeto

A arquitetura do projeto √© dividida em v√°rias partes:

* **`.github`**: diret√≥rio que cont√©m arquivos relacionados √† integra√ß√£o cont√≠nua com o GitHub.
* **`.husky`**: diret√≥rio que cont√©m arquivos relacionados √† configura√ß√£o do Husky, ferramenta que permite a execu√ß√£o de scripts no Git Hooks.
* **`admin`**: diret√≥rio que cont√©m arquivos relacionados √† administra√ß√£o do projeto, como scripts para extra√ß√£o de dados (data_crawlers) e URLs alvo (target_url_crawlers).
* **`docs`**: diret√≥rio que cont√©m arquivos relacionados √† documenta√ß√£o do projeto.
* **`src`**: diret√≥rio que cont√©m o c√≥digo fonte do projeto, organizado em diferentes subdiret√≥rios, como api (que cont√©m as rotas da aplica√ß√£o), data (que cont√©m os arquivos de dados), error (que cont√©m o tratamento de erros), log (que cont√©m os arquivos de logs) e public (que cont√©m arquivos est√°ticos, como imagens).
* **`app.py`**: arquivo que cont√©m a configura√ß√£o e inicializa√ß√£o da aplica√ß√£o Flask.
* **`docker-compose.yml`**: arquivo que cont√©m a configura√ß√£o do Docker Compose para a execu√ß√£o da aplica√ß√£o e do banco de dados.
* **`Dockerfile`**: arquivo que cont√©m a configura√ß√£o do Docker para a constru√ß√£o da imagem da aplica√ß√£o.
* **`Makefile`**: arquivo que cont√©m os comandos de automatiza√ß√£o de tarefas do projeto.
* **`README.md`**: arquivo que cont√©m a descri√ß√£o do projeto e sua documenta√ß√£o.
* **`requirements.txt`**: arquivo que cont√©m as depend√™ncias do projeto.
* **`LICENSE`**: arquivo que cont√©m as informa√ß√µes sobre a licen√ßa do projeto.


## Tecnologias Utilizadas

Neste projeto "Pedacinho do C√©u", diversas tecnologias s√£o utilizadas para coletar, processar, armazenar e visualizar dados. Abaixo est√° uma lista dessas tecnologias e como elas se encaixam no projeto:

1. **Docker Compose**: Ferramenta para definir e gerenciar aplica√ß√µes multi-container usando arquivos de configura√ß√£o (docker-compose.yml). √â usado para simplificar o processo de inicializa√ß√£o e gerenciamento de todos os servi√ßos envolvidos no projeto.

2. **Power BI**: Ferramenta de Business Intelligence (BI) da Microsoft para criar relat√≥rios e visualiza√ß√µes de dados. √â usado para analisar e visualizar os dados coletados e processados pelo projeto.

3. **Flask** (opcional): Microframework Python para desenvolvimento de aplica√ß√µes web. Pode ser usado para criar uma API RESTful que exp√µe os dados processados e armazenados para outras aplica√ß√µes ou servi√ßos.

4. **Matplotlib** (opcional): Biblioteca Python para cria√ß√£o de gr√°ficos e visualiza√ß√µes de dados. Pode ser usada em conjunto com o Apache Spark para criar gr√°ficos e visualiza√ß√µes a partir dos dados processados.

5. **Pandas** (opcional): Biblioteca Python para manipula√ß√£o e an√°lise de dados. Pode ser usada em conjunto com o Apache Spark para realizar an√°lises e limpeza de dados em pequena escala antes de process√°-los no Spark.

6. **Python**: a linguagem de programa√ß√£o utilizada em todas as etapas do projeto, desde a coleta de dados at√© a an√°lise e visualiza√ß√£o. O Python √© uma linguagem de programa√ß√£o interpretada, orientada a objetos e de alto n√≠vel, que possui uma vasta biblioteca padr√£o e diversas bibliotecas de terceiros para processamento de dados.

7. **Streamlit**: uma biblioteca de c√≥digo aberto para cria√ß√£o de aplicativos web de dados em Python. O Streamlit √© utilizado no projeto para criar uma interface amig√°vel e interativa para visualiza√ß√£o dos dados.

8. **Plotly**: uma biblioteca de visualiza√ß√£o de dados interativa de c√≥digo aberto para Python. O Plotly √© utilizado no projeto para criar gr√°ficos e visualiza√ß√µes interativas a partir dos dados processados com o Pandas.

9. **Apache Airflow**: uma plataforma de orquestra√ß√£o de fluxo de trabalho para gerenciamento de tarefas de processamento de dados. O Apache Airflow √© utilizado no projeto para agendar e executar tarefas de coleta, processamento e an√°lise de dados de forma autom√°tica.

10. **Docker**: uma plataforma de c√≥digo aberto para cria√ß√£o, implanta√ß√£o e execu√ß√£o de aplicativos em cont√™ineres. O Docker √© utilizado no projeto para criar e executar cont√™ineres para cada servi√ßo envolvido no projeto.

11. **Apache Spark**: um framework de computa√ß√£o distribu√≠da de c√≥digo aberto para processamento de dados em larga escala. O Apache Spark √© utilizado no projeto para processar e analisar grandes quantidades de dados.

12. **Apache Hadoop**: um framework de computa√ß√£o distribu√≠da de c√≥digo aberto para armazenamento de dados em larga escala. O Apache Hadoop √© utilizado no projeto para armazenar os dados coletados e processados.


## Pr√©-requisitos

* Python 3.6+
* Apache Spark 3.0+
* Power BI Desktop
* Docker
* Docker Compose
* Node.js e npm
* Git
* Insomnia
* IDE de sua prefer√™ncia (PyCharm, VS Code, etc.)
* Terminal de sua prefer√™ncia (Git Bash, PowerShell, etc.)
* Sistema operacional Linux, macOS ou Windows
* Conhecimentos b√°sicos de Python e SQL


## Arquivo de configura√ß√£o package.json

O arquivo `package.json` cont√©m as depend√™ncias do projeto. Para instalar as depend√™ncias, execute o seguinte comando:

```bash
{
  "name": "analise-de-dados",
  "author": "grupo-estacio",
  "version": "1.0.3",
  "main": "./build/electron/main.js",
  "keywords": [
    "restaurante",
    "python",
    "flask"
  ],
  "scripts": {
    "dump": "dump-stlite-desktop-artifacts",
    "dev:app": "streamlit run app.py",
    "production": "NODE_ENV=\"production\" electron .",
    "start": "node ./dist/server.js",
    "servewindows": "electron .",
    "pack": "electron-builder --dir",
    "prisma": "npx prisma",
    "dist": "electron-builder",
    "postinstall": "electron-builder install-app-deps",
    "server": "nodemon --exec npx babel-node client/src/api/router/router.js"
  },
  "build": {
    "files": ["build/**/*"],
    "directories": {
      "buildResources": "assets"
    },
    "win": {
      "target": "portable",
      "icon": "assets/icon.ico"
    }
  },
  "dependencies": {
    "-": "^0.0.1",
    "@prisma/client": "^4.13.0",
    "@types/multer": "^1.4.7",
    "D": "^1.0.0",
    "cors": "^2.8.5",
    "csv-parser": "^3.0.0",
    "csv-writer": "^1.6.0",
    "express": "^4.18.2",
    "multer": "^1.4.5-lts.1",
    "node-fetch": "^3.3.1",
    "typescript": "^5.0.4"
  },
  "devDependencies": {
    "@stlite/desktop": "^0.22.2",
    "electron": "22.0.0",
    "electron-builder": "^23.6.0",
    "@babel/core": "^7.21.5",
    "@babel/node": "^7.20.7",
    "@types/cors": "^2.8.13",
    "@types/express": "^4.17.17",
    "prisma": "^4.13.0",
    "nodemon": "^2.0.22"
  }
}
```


## Instalando o Projeto

1. Clone o reposit√≥rio:

```bash
git clone https://github.com/big-data-estacio/data.git
cd data
```


## Executando o Projeto

### Configurando o ambiente virtual

√â recomendado utilizar um ambiente virtual para isolar as depend√™ncias do projeto. Siga os passos abaixo para configurar e ativar o ambiente virtual usando o `venv`:

1. Instale o m√≥dulo `venv`, caso ainda n√£o tenha, com o seguinte comando:

```bash
python3 -m pip install --user virtualenv
```

2. Navegue at√© a pasta do projeto e crie um ambiente virtual:

```bash
virtualenv myenv

# ou, se voc√™ estiver no Windows

py -m venv myenv
```

3. Ative o ambiente virtual:

* No Windows:

```bash
.\myenv\Scripts\activate

# ou, se voc√™ estiver usando o Git Bash

source myenv/Scripts/activate
```

* No macOS e Linux:

```bash
set -x VIRTUAL_ENV /mnt/c/Users/estev/OneDrive/√Årea de Trabalho/johanEstevam/myenv
source myenv/bin/activate.fish
```

4. Ap√≥s a ativa√ß√£o do ambiente virtual, seu terminal deve mostrar o prefixo `(venv)`.

Agora voc√™ pode executar o projeto com as depend√™ncias instaladas no ambiente virtual. Lembre-se de ativar o ambiente virtual sempre que for trabalhar no projeto.

5. Instale todas as bibliotecas necess√°rias usando o comando `pip install`:

```bash
pip install biblioteca1 biblioteca2 biblioteca3
```

Substitua `biblioteca1`, `biblioteca2` e `biblioteca3` pelos nomes das bibliotecas que voc√™ deseja instalar.

6. Agora que todas as bibliotecas est√£o instaladas, execute o seguinte comando para gerar o arquivo `requirements.txt`:

```bash
pip freeze > requirements.txt
```

O comando `pip freeze` listar√° todas as bibliotecas instaladas no ambiente virtual e suas vers√µes espec√≠ficas. O operador `>` redirecionar√° a sa√≠da para o arquivo `requirements.txt`, criando-o ou atualizando-o, se j√° existir.

7. Instale as depend√™ncias do projeto utilizando o arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

> arquivo requirements.txt para esse projeto com as depend√™ncias necess√°rias.

```bash
altair==4.2.2
attrs==23.1.0
bcrypt==4.0.1
beautifulsoup4==4.12.2
blinker==1.6.2
bs4==0.0.1
cachetools==5.3.0
certifi==2022.12.7
chardet==3.0.4
charset-normalizer==3.1.0
click==8.1.3
cssselect==1.2.0
cssutils==2.6.0
cycler==0.11.0
decorator==5.1.1
deta==1.1.0
entrypoints==0.4
extra-streamlit-components==0.1.56
findspark==1.4.2
gitdb==4.0.10
GitPython==3.1.31
googletrans==3.0.0
h11==0.9.0
h2==3.2.0
hpack==3.0.0
hstspreload==2023.1.1
httpcore==0.9.1
httpx==0.13.3
hydralit-components==1.0.10
hyperframe==5.2.0
idna==2.10
importlib-metadata==6.6.0
Jinja2==3.1.2
jsonschema==4.17.3
kiwisolver==1.4.4
lxml==4.9.2
markdown-it-py==2.2.0
MarkupSafe==2.1.2
matplotlib==3.4.3
mdurl==0.1.2
numpy==1.21.0
packaging==23.1
pandas==1.3.5
Pillow==9.5.0
plotly==5.3.1
premailer==3.10.0
protobuf==3.20.3
psutil==5.8.0
py4j==0.10.9.2
pyarrow==12.0.0
pydeck==0.8.1b0
Pygments==2.15.1
PyJWT==2.6.0
Pympler==1.0.1
pyparsing==3.0.9
pyrsistent==0.19.3
pyspark==3.2.0
python-dateutil==2.8.2
python-dotenv==0.17.1
pytz==2023.3
pytz-deprecation-shim==0.1.0.post0
PyYAML==6.0
requests==2.29.0
rfc3986==1.5.0
rich==13.3.5
six==1.16.0
smmap==5.0.0
sniffio==1.3.0
soupsieve==2.4.1
streamlit==1.22.0
streamlit-authenticator==0.2.1
streamlit-lottie==0.0.3
tenacity==8.2.2
toml==0.10.2
toolz==0.12.0
tornado==6.0.4
typing_extensions==4.5.0
tzdata==2023.3
tzlocal==4.3
urllib3==1.26.15
validators==0.20.0
watchdog==3.0.0
yagmail==0.14.260
zipp==3.15.0
```

8. Quando terminar de trabalhar no projeto, voc√™ pode desativar o ambiente virtual com o seguinte comando:

```bash
deactivate
```

### Continuando a instala√ß√£o

1. Crie um arquivo `.env` na raiz do projeto com as vari√°veis de ambiente necess√°rias. Voc√™ pode usar o arquivo `.env.example` como modelo.

```bash
cp .env.example .env
```

> O c√≥digo acima copiar√° o arquivo `.env.example` e o renomear√° para `.env` e voc√™ poder√° preencher as vari√°veis de ambiente necess√°rias.
> *Nota: O arquivo `.env` √© ignorado pelo Git, portanto, n√£o ser√° enviado para o reposit√≥rio.*

2. Instale as depend√™ncias do projeto com o yarn na raiz do projeto:

```bash
yarn add
```


### Para iniciar o projeto "Pedacinho do C√©u" com o Docker, siga estas etapas:

1. Certifique-se de que o Docker e o Docker Compose estejam instalados em seu sistema. Se voc√™ ainda n√£o os instalou, siga as instru√ß√µes de instala√ß√£o no site oficial do Docker: `https://docs.docker.com/get-docker/` e `https://docs.docker.com/compose/install/`

2. Abra um terminal e navegue at√© o diret√≥rio raiz do projeto `data`.

3. Execute o seguinte comando para criar e iniciar os containers do projeto, conforme definido no arquivo `docker-compose.yml`:

<!-- - ```docker build -t streamlit-ts-ml:0.1.0 -f Dockerfile .```
- ```docker run -p 8501:8501 streamlit-ts-ml:0.1.0``` -->
- ```docker build -t big_data_app_estacio .```
- ```docker run -p 8501:8501 -e MYSQL_USER=user -e MYSQL_PASSWORD=user -e MYSQL_DATABASE=big_data_app_estacio -e MYSQL_HOST=192.168.1.100 big_data_app_estacio```
- Open `http://localhost:8501/`
- ou
- acesse o app do docker e execute o container
![docker](client/src/public/docker.jpeg)
<!-- - Build with ```docker build -t ts-forecast-app .``` (takes some time!)
- Run with ```docker run -p 8501:8501 ts-forecast-app:latest``` -->

Isso iniciar√° os servi√ßos do Docker, a API, como o MySQL e o servi√ßo de processamento de dados se necess√°rio.

4. Abra os relat√≥rios do Power BI na pasta `data_visualization/power_bi_reports/` para visualizar e analisar os dados processados. Se voc√™ n√£o possui o Power BI Desktop, fa√ßa o download e instale-o a partir do site oficial: `https://powerbi.microsoft.com/en-us/desktop/`

5. Ao longo do desenvolvimento do projeto, voc√™ pode modificar e ajustar os arquivos na pasta `src/` conforme necess√°rio para aprimorar a an√°lise e o processamento dos dados.

Lembre-se de que, dependendo da configura√ß√£o do projeto, algumas etapas podem variar ou exigir a√ß√µes adicionais. Ajuste as etapas conforme necess√°rio para se adequar √†s necessidades espec√≠ficas do seu projeto.

6. Quando terminar de usar o projeto, pare os servi√ßos do Docker Compose pressionando `Ctrl+C` no terminal onde o Docker Compose est√° sendo executado. Para remover os cont√™ineres e os volumes, execute:

```bash
docker-compose down --remove-orphans --volumes
```

### Para iniciar o projeto "Pedacinho do C√©u" sem o Docker, siga estas etapas:

1. Abra um terminal e navegue at√© o diret√≥rio raiz do projeto `data`.

2. Execute o seguinte comando para executar o projeto na pasta `client`:

```bash
cd client
yarn run server:app
```

# ou

```bash
cd [client]
streamlit run app.py --server.address 0.0.0.0 --server.port <your port>
# http://0.0.0.0:[your port]
```

### Iniciando a API

1. Inicie o servidor da API:
  
```bash
node client/src/api/index.js
```

> J√° o arquivo api/app.py, √© o arquivo principal da API, onde √© feita a conex√£o com o banco de dados e a defini√ß√£o dos endpoints.

Ele √© executado na porta 5000, e pode ser acessado em `http://localhost:5000/`

### Demo

A demonstra√ß√£o do projeto pode ser acessada em `https://pedacinho-do-ceu.herokuapp.com/`

<video width="700" height="400" controls>
  <source src="client/src/public/streamlit-app.mp4" type="video/mp4">
  Seu navegador n√£o suporta a tag de v√≠deo.
</video>

### Acessando as funcionalidades do projeto

* **`/`**: P√°gina inicial do projeto, onde √© poss√≠vel visualizar os dados processados e analisados.
* **`/about`**: P√°gina com informa√ß√µes sobre o projeto e os desenvolvedores.
* **`/data`**: P√°gina com informa√ß√µes sobre os dados utilizados no projeto.
* **`/data-visualization`**: P√°gina com informa√ß√µes sobre a visualiza√ß√£o dos dados.
* **`/data-processing`**: P√°gina com informa√ß√µes sobre o processamento dos dados.


## Utilizando a API

A API RESTful permite gerenciar os dados armazenados no banco de dados PostgreSQL. Para utilizar a API, voc√™ pode fazer requisi√ß√µes HTTP para os seguintes endpoints:

Nesse arquivo, se encontram os endpoints da API, que s√£o:

* **`GET /`**: Lista todos os itens armazenados no banco de dados.
* **`GET /<id>`**: Retorna os detalhes de um item espec√≠fico com base no ID.
* **`POST /`**: Adiciona um novo item ao banco de dados.
* **`PUT /<id>`**: Atualiza os detalhes de um item espec√≠fico com base no ID.
* **`DELETE /<id>`**: Remove um item espec√≠fico com base no ID.

> @TODO: Adicionar mais detalhes sobre a API
</br>
> @Note: A API foi desenvolvida com o framework FastAPI, que √© um framework web ass√≠ncrono de alto desempenho, f√°cil de aprender, r√°pido para codificar, pronto para produ√ß√£o. Ele √© constru√≠do com base no Starlette e Pydantic.

o arquivo `api/app.py` est√° configurado com os scripts de coleta de dados e processamento de dados, para que os dados sejam coletados e processados automaticamente quando a API for iniciada:

```python
from flask import Flask
from routes.bebidas import bebidas_bp
from routes.clientes import clientes_bp

app = Flask(__name__)
app.register_blueprint(bebidas_bp)
app.register_blueprint(clientes_bp)

if __name__ == '__main__':
    app.run(debug=True)
```

A API estar√° dispon√≠vel na porta 5000.

* Abra o Power BI Desktop e carregue os relat√≥rios na pasta `data_visualization/power_bi_reports`.


## Testes

Execute os testes unit√°rios e de integra√ß√£o usando o seguinte comando:

```bash
python -m unittest discover tests
```


## Debugging e Logging

Para depurar o projeto, voc√™ pode usar o depurador interativo do Python. Para isso, basta adicionar o seguinte c√≥digo ao arquivo que voc√™ deseja depurar:

```python
import pdb; pdb.set_trace()
```

Em seguida, execute o arquivo normalmente e o depurador interativo ser√° iniciado quando o c√≥digo atingir o ponto de interrup√ß√£o.

Para adicionar logs ao projeto, voc√™ pode usar o m√≥dulo `logging` do Python. Para isso, basta adicionar o seguinte c√≥digo ao arquivo que voc√™ deseja depurar:

```python
import logging

logging.basicConfig(level=logging.DEBUG)
```

> como que fica o arquivo de log logs/app.log

<details>
  <summary>Exemplo de arquivo de log</summary

```log
2023-04-25 00:31:31,719 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:33:49,546 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:34:44,584 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:34:46,857 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 00:34:46,859 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 00:34:46,860 - INFO - Probing node bootstrap-0 broker version
2023-04-25 00:34:47,912 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 00:34:47,915 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 00:36:12,026 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:37:32,931 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:38:27,387 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:38:29,541 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 09:38:29,542 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 09:38:29,542 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:38:30,582 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 09:38:30,582 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 09:39:43,938 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:39:44,780 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:39:44,780 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:39:44,781 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:39:44,782 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:39:44,827 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:39:44,828 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:39:44,828 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:41:52,885 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:41:53,647 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:41:53,648 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:41:53,649 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:41:53,649 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:41:53,693 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:41:53,694 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:41:53,694 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:42:12,859 - INFO - A aplica√ß√£o foi encerrada com sucesso.
```

</details>

<details>
  <summary>O arquivo de debug:</summary>

[![debug](client/src/public/debug.png)](client/src/public/debug.png)

</details>

## Utilizando a API com Insomnia

Para testar e interagir com a API, voc√™ pode usar o Insomnia REST Client, que √© uma aplica√ß√£o de desktop para gerenciar requisi√ß√µes HTTP e API.

1. Instale o Insomnia em seu computador: Insomnia Download
2. Abra o Insomnia e crie um novo workspace ou use um existente.
3. Para adicionar as rotas da API ao seu workspace, clique em `Create Request`. Escolha o m√©todo HTTP (GET, POST, PUT, DELETE) e insira a URL da API (por exemplo, `http://localhost:5000/` para listar todos os itens). Adicione os par√¢metros, cabe√ßalhos ou corpo da requisi√ß√£o conforme necess√°rio e clique em `Send` para executar a requisi√ß√£o.
4. Voc√™ pode criar v√°rias requisi√ß√µes para diferentes endpoints da API e organiz√°-las em pastas para facilitar a navega√ß√£o.

Agora voc√™ pode usar o Insomnia para testar e explorar a API RESTful do projeto Pedacinho do C√©u.


## Vers√£o atual
A vers√£o atual do projeto inclui as seguintes funcionalidades:

* Coleta e armazenamento de dados em tempo real com Hadoop e Kafka.
* API RESTful para interagir com os dados e servi√ßos.
* Docker Compose para facilitar a instala√ß√£o e configura√ß√£o do projeto.


## Coletando Dados

Os dados podem ser coletados usando o script `data_collection/data_collector.py`. Para coletar os dados, voc√™ pode executar o seguinte comando:

```bash
python data_collection/data_collector.py
```

Os dados coletados ser√£o armazenados em arquivos no diret√≥rio `data_collection/data`.


## Processando Dados

Os dados podem ser processados usando o script `data_processing/data_processor.py`. Para processar os dados, voc√™ pode executar o seguinte comando:

```bash
python data_processing/data_processor.py
```

Os dados processados ser√£o armazenados no banco de dados PostgreSQL.


## Visualizando os Dados

Os dados podem ser visualizados usando relat√≥rios do Power BI. Para visualizar os relat√≥rios, voc√™ pode fazer o download do Power BI Desktop [aqui](https://powerbi.microsoft.com/pt-br/desktop/). Ap√≥s instalar o Power BI Desktop, voc√™ pode abrir o arquivo `data.pbix` para visualizar os relat√≥rios.


## Futuras Atualiza√ß√µes

* Melhoria na coleta de dados
* Implementa√ß√£o de novas an√°lises e visualiza√ß√µes
* Adi√ß√£o de mais testes
* Melhoria na documenta√ß√£o

O projeto "Pedacinho do C√©u" est√° em constante evolu√ß√£o e melhorias. Abaixo est√£o algumas das atualiza√ß√µes futuras planejadas para o projeto:

1. **Expans√£o das fontes de dados**: Atualmente, o projeto utiliza um conjunto limitado de fontes de dados. No futuro, pretendemos expandir a lista de fontes de dados para enriquecer ainda mais nossas an√°lises e visualiza√ß√µes.

2. **Otimiza√ß√£o do processamento de Big Data**: Continuaremos a melhorar a efici√™ncia e a velocidade do processamento de Big Data usando t√©cnicas de otimiza√ß√£o avan√ßadas e atualiza√ß√µes nas ferramentas e tecnologias utilizadas.

3. **Visualiza√ß√µes interativas**: Atualmente, o projeto utiliza relat√≥rios est√°ticos do Power BI. Planejamos adicionar visualiza√ß√µes interativas e em tempo real para tornar os dados mais acess√≠veis e f√°ceis de entender.

4. **Machine Learning**: Exploraremos a implementa√ß√£o de algoritmos de aprendizado de m√°quina para identificar padr√µes e tend√™ncias nos dados e fornecer insights adicionais.

5. **Integra√ß√£o com outras plataformas**: Planejamos adicionar suporte para integra√ß√£o com outras plataformas de visualiza√ß√£o de dados e an√°lise, como Tableau e Looker.

6. **APIs e microservi√ßos**: Atualmente, o projeto possui uma API simples para acessar os dados. No futuro, pretendemos expandir a API e usar microservi√ßos para fornecer uma solu√ß√£o mais modular e escalon√°vel.

7. **Seguran√ßa e privacidade**: Continuaremos a aprimorar a seguran√ßa e a privacidade dos dados, garantindo a conformidade com os regulamentos de prote√ß√£o de dados e as melhores pr√°ticas do setor.

8. **Documenta√ß√£o e tutoriais**: A documenta√ß√£o e os tutoriais ser√£o atualizados regularmente para garantir que sejam claros, abrangentes e atualizados com as √∫ltimas informa√ß√µes do projeto.


## Tecnologias e conceitos utilizados

### Docker
O projeto √© executado em cont√™ineres Docker para facilitar a implanta√ß√£o e a escalabilidade.

### SOLID
Os princ√≠pios SOLID foram aplicados neste projeto para garantir um c√≥digo limpo, modular e f√°cil de manter.

## Padr√µes de commit
Ao fazer um commit, siga as diretrizes de mensagens de commit personalizadas e com emojis, utilizando at√© 200 caracteres.


## Roadmap v1

* Adicionar suporte ao Travis CI para integra√ß√£o cont√≠nua.
* Aplicar os princ√≠pios SOLID ao projeto para melhorar a manutenibilidade e escalabilidade.
* Atualizar o SQLAlchemy para usar um banco de dados mais robusto, como PostgreSQL ou MySQL, em vez de SQLite.
* Implementar autentica√ß√£o e autoriza√ß√£o para proteger as rotas da API.
* Adicionar testes automatizados para garantir a qualidade do c√≥digo.
* Implementar uma interface de usu√°rio para interagir com a API.

### Travis CI

1. Para adicionar suporte ao Travis CI, siga estas etapas:
2. V√° para `travis-ci.com` e fa√ßa login com sua conta do GitHub.
3. Ative o reposit√≥rio do projeto nas configura√ß√µes do Travis CI.
4. Crie um arquivo `.travis.yml` na raiz do projeto com o seguinte conte√∫do:

```yml
language: python
python:
  - "3.8"

install:
  - pip install -r api/requirements.txt

script:
  - pytest
```

5. Vincule seu reposit√≥rio do GitHub ao Travis CI e habilite a integra√ß√£o cont√≠nua para o projeto.


### Princ√≠pios SOLID
Para aplicar os princ√≠pios SOLID ao projeto, siga estas diretrizes:

1. Single Responsibility Principle (SRP): Separe as responsabilidades do c√≥digo em classes e fun√ß√µes distintas. Por exemplo, separe a l√≥gica de neg√≥cio da l√≥gica de banco de dados.

2. Open/Closed Principle (OCP): Crie classes e fun√ß√µes extens√≠veis sem modificar seu c√≥digo-fonte. Por exemplo, use a heran√ßa e a composi√ß√£o para estender a funcionalidade de uma classe.

3. Liskov Substitution Principle (LSP): As subclasses devem ser substitu√≠veis por suas classes base sem alterar a corre√ß√£o do programa. Por exemplo, verifique se as subclasses seguem o contrato estabelecido pelas classes base.

4. Interface Segregation Principle (ISP): Divida as interfaces grandes em interfaces menores e mais espec√≠ficas. Por exemplo, crie interfaces separadas para opera√ß√µes de leitura e grava√ß√£o em vez de uma interface √∫nica.

5. Dependency Inversion Principle (DIP): Depend√™ncias de alto n√≠vel n√£o devem depender de depend√™ncias de baixo n√≠vel; em vez disso, ambas devem depender de abstra√ß√µes. Por exemplo, use a inje√ß√£o de depend√™ncia para inverter as depend√™ncias entre os m√≥dulos.

### SQLAlchemy com PostgreSQL ou MySQL
Para usar o PostgreSQL ou MySQL com SQLAlchemy, siga estas etapas:

1. Instale os pacotes necess√°rios:

```bash
pip install psycopg2 sqlalchemy psycopg2-binary
```

OU

```bash
pip install pymysql sqlalchemy
```

2. Atualize a string de conex√£o do banco de dados no arquivo api/database.py para usar PostgreSQL ou MySQL:

Para PostgreSQL:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "postgresql://user:password@localhost/dbname"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

Para MySQL:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "mysql+pymysql://user:password@localhost/dbname"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

3. Atualize as configura√ß√µes do banco de dados em `api/__init__.py` para usar a nova string de conex√£o.

### Autentica√ß√£o e autoriza√ß√£o
Para implementar a autentica√ß√£o e autoriza√ß√£o, voc√™ pode usar a extens√£o Flask-Login ou Flask-Security. Siga a documenta√ß√£o oficial de cada extens√£o para configurar a autentica√ß√£o e autoriza√ß√£o no projeto.

### Testes automatizados
Para adicionar testes automatizados, siga estas etapas:

1. Instale o pacote pytest:
  
```bash
pip install pytest
```

2. Crie um diret√≥rio `tests` na raiz do projeto.

3. Dentro do diret√≥rio `tests`, crie arquivos de teste para cada m√≥dulo do projeto. Por exemplo, voc√™ pode criar um arquivo `test_restaurant_service.py` para testar a classe RestaurantService.

4. Escreva testes usando o pytest para cada fun√ß√£o e classe do projeto.

5. Execute os testes usando o comando `pytest` no terminal.

### Interface de usu√°rio
Para implementar uma interface de usu√°rio, voc√™ pode criar uma aplica√ß√£o web usando HTML, CSS e JavaScript para interagir com a API. Voc√™ pode usar bibliotecas como React, Angular ou Vue.js para criar a interface do usu√°rio.


## Roadmap v2

### Vers√£o 1.1
* Implementar autentica√ß√£o e autoriza√ß√£o na API.
* Adicionar suporte para diferentes formatos de dados (JSON, XML, etc.).
* Desenvolver uma interface gr√°fica para visualizar os dados e interagir com a API.

### Vers√£o 1.2
* Adicionar suporte para processamento de dados em tempo real usando Apache Spark ou Flink.
* Implementar algoritmos de machine learning para an√°lise de dados e gera√ß√£o de insights.
* Integrar com outras fontes de dados e APIs de terceiros.

### Vers√£o 1.3
* Adicionar suporte para escalabilidade horizontal e balanceamento de carga.
* Implementar monitoramento e alertas para garantir a sa√∫de e o desempenho do sistema.
* Desenvolver testes de carga e estresse para garantir a estabilidade do projeto em ambientes de produ√ß√£o.

### Vers√£o 2.0
* Redesenhar a arquitetura do projeto para melhorar a modularidade e a manutenibilidade.
* Adicionar suporte para outras linguagens de programa√ß√£o e frameworks.
* Implementar uma API GraphQL para maior flexibilidade na consulta e manipula√ß√£o de dados.


## O que √© o Apache Spark?

![](client/src/public/spark-streaming-datanami.png)
# Apache_Spark
:sparkles::tada: Este reposit√≥rio explica sobre o Apache Spark com pr√°ticas :tada::sparkles:

- √â uma ferramenta de processamento em tempo real desenvolvida para resolver o problema de trabalhar com dados em tempo real.
- √â tratado pelo Apache.
- √â muito r√°pido, d√° resultado no clique.
- Usa avalia√ß√£o pregui√ßosa, ou seja, processa sempre que necess√°rio.
- Como o MapReduce n√£o conseguiu lidar com dados em tempo real, o Spark entrou em cena para ajudar.
- Agora √© usado por muitos grandes gigantes da tecnologia como Oracle, Amazon, Microsoft, Visa, Cisco, Verizon, Hortonworks.
- Como acima, temos 3.000 empresas usando o Apache Spark.

<div align="center">
  <a href="https://www.youtube.com/watch?v=4TE6AGQ0IzI" target="_blank"
  alt="IMAGE ALT TEXT" width="20" height="10" border="10"><img src="https://i.ytimg.com/vi/4TE6AGQ0IzI/maxresdefault.jpg"></a>
</div>

# Para iniciar o cluster Spark a partir da m√°quina local com o Linux, voc√™ precisa seguir os seguintes passos:


## **Instala√ß√£o e Execu√ß√£o do Apache Spark no Ubuntu/Debian/Linux Mint/Pop!_OS/Elementary OS/Linux Lite/Peppermint OS/Windows 10 com WSL**

Este guia tem como objetivo fornecer os passos necess√°rios para a instala√ß√£o e execu√ß√£o do Apache Spark em um ambiente Linux. O Apache Spark √© um sistema de computa√ß√£o distribu√≠da que permite processar grandes volumes de dados em um cluster de computadores. Ele √© amplamente utilizado para an√°lise de big data, machine learning, processamento de stream e outras aplica√ß√µes de processamento paralelo.

<div align="center">
  <a href="https://www.youtube.com/watch?v=DZ-ciI5_CQw" target="_blank"
  alt="IMAGE ALT TEXT" width="20" height="10" border="10"><img src="https://i.ytimg.com/vi/DZ-ciI5_CQw/maxresdefault.jpg"></a>
</div>

## Pr√©-requisitos
--------------

Antes de come√ßar a instalar o Apache Spark, certifique-se de que o seu sistema atenda aos seguintes requisitos:

*   Linux (Ubuntu ou outra distribui√ß√£o)
*   Java 8 ou superior
*   Python 2.7 ou 3.x
*   Acesso √† Internet
- Um sistema operacional Debian ou seus derivados.
- Acesso √† linha ou terminal de comando.
- Um usu√°rio com permiss√µes sudo ou ** root**.

Antes de baixar e configurar o Spark, precisamos instalar depend√™ncias. Essas etapas incluem a instala√ß√£o da seguinte parcela.

- JDK
- Scala
- Git


> Certifique-se de que o Java esteja instalado em sua m√°quina.

* Instale o Java 8 caso n√£o esteja instalado:

Abra o terminal e execute o seguinte comando: `sudo apt update && sudo apt install openjdk-8-jdk`

* Verifique a vers√£o do Python instalada:

```bash
python --version
```

* Instale o Python 3.7 ou superior caso n√£o esteja instalado:

```bash
sudo apt update
sudo apt install python3
```


### 1\. Verifique se o Java est√° instalado

Antes de come√ßar a instalar o Apache Spark, verifique se o Java est√° instalado em sua m√°quina. O Apache Spark requer que o Java esteja instalado.

Para verificar a vers√£o do Java em sua m√°quina, execute o seguinte comando em seu terminal:

`java -version`

Se o Java n√£o estiver instalado, siga as instru√ß√µes no site oficial da Oracle para baixar e instalar o Java.


### Instala√ß√£o por gerenciador de pacotes

1.  Abra o terminal e execute o seguinte comando para instalar o Apache Spark:
    
    sql
    
    ```sql
    sudo apt update
    sudo apt install spark
    ```

### Abra o navegador e digite o seguinte URL na barra de endere√ßo para instalar o Apache Spark:

Agora, precisamos baixar a vers√£o do Spark que voc√™ deseja e ela est√° dispon√≠vel no site oficial. No momento da edi√ß√£o deste texto, a vers√£o mais atualizada √© _Spark 3.2.1_ ( Enero-26-2022 ) junto com o pacote _Hadoop 3.2_.

Usamos o comando ** wget ** junto com o link do site para baixar nosso arquivo Spark:

```bash
https://www.apache.org/dyn/closer.lua/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
```

***
**Nota: ** Se o URL n√£o for executado, v√° para a p√°gina oficial [ Apache Spark ] ( https://spark.apache.org/) e procure a vers√£o mais recente no menu de download ( ). Em outras palavras, voc√™ tamb√©m pode tentar substituir as vers√µes da liga que estou compartilhando com voc√™.
***

* logo em seguida baixe na raiz do projeto o arquivo spark-3.4.0-bin-hadoop3.tgz
* mova o arquivo para a pasta /opt:

```bash
sudo mv spark-3.4.0-bin-hadoop3.tgz /opt
```

* acessar a pasta /opt:

```bash
cd /opt
```

* descompactar o arquivo:

```bash
sudo tar -xvzf spark-3.4.0-bin-hadoop3.tgz
```

* acessar a pasta spark-3.4.0-bin-hadoop3:

```bash
cd spark-3.4.0-bin-hadoop3
```

* configurar as vari√°veis de ambiente:

```bash
sudo nano /etc/profile
```


## Configure o ambiente Spark

Antes de inicializar o servidor mestre, precisamos configurar as vari√°veis de ambiente. Esses s√£o frequentemente caminhos ( ) no Spark que precisamos adicionar ao perfil do usu√°rio.


Tamb√©m podemos adicionar as rotas de exporta√ß√£o editando o arquivo _.profile_ no editor escolhido, como nano ou vim.

Por exemplo, para o nano editor, inserimos:

```javascript
nano ~ / .profile
```

Ao carregar o perfil, colocamos no final do arquivo:

![ ](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/export_paths.png)

Ent√£o, adicionamos as duas linhas a seguir no final do arquivo:

```bash
export SPARK_HOME=/opt/spark-3.4.0-bin-hadoop3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```


* salvar o arquivo e sair do editor de texto com os seguntes comandos:

```bash
CTRL + s
CTRL + x
```

10. Execute o comando abaixo para atualizar as vari√°veis de ambiente:

```bash
bash -n ~/.bashrc
```

10. Verifique se o Apache Spark foi instalado corretamente:

```bash
spark-submit --version
```

> Caso ocorra esse erro:

```log
Traceback (most recent call last):
  File "/home/estevam/micromamba/bin/find_spark_home.py", line 95, in <module>
    print(_find_spark_home())
  File "/home/estevam/micromamba/bin/find_spark_home.py", line 59, in _find_spark_home
    module_home = os.path.dirname(find_spec("pyspark").origin)
AttributeError: 'NoneType' object has no attribute 'origin'
/home/estevam/micromamba/bin/spark-submit: line 27: /bin/spark-class: No such file or directory
```


Esse erro indica que o PySpark n√£o foi instalado corretamente ou n√£o foi encontrado no sistema.

Verifique se voc√™ seguiu todas as etapas corretamente durante a instala√ß√£o e se adicionou as vari√°veis de ambiente corretamente.

Voc√™ pode tentar reinstalar o PySpark usando o comando pip install pyspark ou verifique se o caminho para o Spark Home est√° correto e atualizado nas vari√°veis de ambiente.

> Agora poder√° usar novamente o comando ```spark-submit --version``` e logo aparecer√° isso no terminal:

```log
23/05/02 18:52:38 WARN Utils: Your hostname, estevam resolves to a loopback address: 127.0.1.1; using 172.20.231.90 instead (on interface eth0)
23/05/02 18:52:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0
      /_/
                        
Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 11.0.18
Branch HEAD
Compiled by user xinrong.meng on 2023-04-07T02:18:01Z
Revision 87a5442f7ed96b11051d8a9333476d080054e5a0
Url https://github.com/apache/spark
Type --help for more information.
```

> rode agora o ```pyspark``` no terminal e ver√° essa informa√ß√£o:

```log
Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) 
[GCC 11.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
23/05/02 18:50:48 WARN Utils: Your hostname, estevam resolves to a loopback address: 127.0.1.1; using 172.20.231.90 instead (on interface eth0)
23/05/02 18:50:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/05/02 18:50:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.0
      /_/

Using Python version 3.9.16 (main, Feb  1 2023 21:39:03)
Spark context Web UI available at http://172.20.231.90:4040
Spark context available as 'sc' (master = local[*], app id = local-1683064253636).
SparkSession available as 'spark'.
>>> 
```


Agora que o PySpark est√° rodando corretamente no seu ambiente, voc√™ pode seguir as etapas do roadmap que voc√™ criou anteriormente para executar o seu projeto.

Voc√™ pode, por exemplo, utilizar as fun√ß√µes que voc√™ criou anteriormente para ler os arquivos estoque_mercadorias.csv e total_clientes.csv, realizar transforma√ß√µes e an√°lises nos dados utilizando o PySpark e gerar os gr√°ficos utilizando o Altair.

Lembre-se de ajustar as fun√ß√µes para que elas utilizem o PySpark ao inv√©s do Pandas para processar os dados. Isso pode envolver a utiliza√ß√£o de opera√ß√µes espec√≠ficas do PySpark como o groupBy e o agg.

## Inicializando o Servidor Mestre de spark Independente

Agora que conclu√≠mos as configura√ß√µes do ambiente Spark, podemos inicializar o servidor mestre.

No terminal, escrevemos:

```bash
sudo /opt/spark-3.4.0-bin-hadoop3/sbin/start-master.sh
```

* acessar o endere√ßo http://localhost:8080/ para verificar se o servidor est√° funcionando

Para visualizar a interface da web Spark, abrimos um navegador da web e inserimos o endere√ßo IP do host local na porta 8080.

```http://127.0.0.1:8080/``` ou ```http://localhost:8080/```

* **OBS:** caso o servidor n√£o esteja funcionando, verifique se a porta 8080 est√° liberada no firewall


A p√°gina mostra seu ** URL Spark **, informa√ß√µes sobre o status do trabalhador ( trabalhadores ), recursos de hardware usados etc.

![ ](client/src/public/apache.png)

O URL do Master Spark √© o nome do seu dispositivo na porta 8080. No meu caso, √© _ ** debian.gabi: 8080 ** _. Ent√£o, aqui temos tr√™s maneiras poss√≠veis de carregar a UI Spark Master Web:

1. 127.0.0.1: 8080
2. localhost: 8080
3. nome do dispositivo: 8080


## Inicialize o servidor do trabalhador Spark ( inicie um processo do trabalhador )

Nesta configura√ß√£o aut√¥noma de um √∫nico servidor, inicializaremos um servidor em funcionamento junto com o servidor mestre.

Para isso, executamos o seguinte comando neste formato:

```javascript
start-slave.sh spark: // master: port
```

O ** master ** neste comando pode ser um IP ou um nome de host.

No meu caso, √© ** debian.gabi:**

```javascript
start-slave.sh spark://estevam.localdomain:7077
```

Agora que o trabalhador ou slave est√° carregado e funcionando, se recarregarmos a interface do usu√°rio da Web do Spark Master, voc√™ dever√° v√™-la na lista:

![ ](client/src/public/sparkWorker.png)

### Especifique a aloca√ß√£o de recursos para trabalhadores

A configura√ß√£o padr√£o quando inicializamos um trabalhador em uma m√°quina √© a dispon√≠vel por todos os n√∫cleos da CPU. Voc√™ pode especificar o n√∫mero de n√∫cleos que passam pelos sinalizadores ** -c ** para o comando ** start-slave**.

```javascript
start-slave.sh -c 1 spark: //estevam.localdomain:7077
```

Recarregamos a interface do usu√°rio da Web do Spark Master para confirmar as configura√ß√µes do trabalhador.

![ ](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/core.png)

Da mesma forma, podemos alocar a quantidade espec√≠fica de mem√≥ria quando inicializamos o trabalhador. A configura√ß√£o padr√£o √© a quantidade de mem√≥ria RAM usada pela m√°quina menos 1 GB.

Ao iniciar um trabalhador e atribuir a ele uma quantidade espec√≠fica de mem√≥ria, adicionamos a op√ß√£o ** -m ** e o n√∫mero. Para gigabytes, usamos ** G ** e para megabytes, usamos ** M.**

Por exemplo, para iniciar um trabalhador com 512 MB de mem√≥ria, inserimos o seguinte comando:

```javascript
start-slave.sh -m 512MB spark://estevam.localdomain:7077
```

Recarregue a interface do usu√°rio da Web do Spark Master para visualizar o status do trabalhador e confirmar as configura√ß√µes 

![ ](client/src/public/apacheSparkWorker.png)

## Tente Spark Shell

Depois de concluirmos a configura√ß√£o e inicializa√ß√£o do servidor mestre e slave, testamos se o shell Spark funciona.

Carregamos o shell digitando:

```javascript
spark-shell
```

Voc√™ precisar√° obter uma notifica√ß√£o na tela e a descri√ß√£o do Spark. Por padr√£o, a interface √© Scala e carrega o shell quando voc√™ executa _spark-shell_.

No final da sa√≠da, a imagem aparecer√° com a vers√£o usada ao escrever este guia:

![ ](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/spark_shell.png)

## Testando Python no Spark

Se voc√™ n√£o quiser usar a interface Scala por padr√£o, poder√° usar o Python.

Certifique-se de sair do Scala e, em seguida, inserimos o seguinte comando:

```javascript
pyspark
```

A sa√≠da resultante √© semelhante √† anterior. Na parte inferior, voc√™ ver√° a vers√£o Python.

![ ](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/pyspark.png)

Para sair deste shell, digite ***exit( )*** e pressione Enter.

## Comandos b√°sicos para iniciar e parar o servidor Master e Escalavos ( Servidor mestre e de trabalhadores ) 

Voc√™ encontrar√° imediatamente os comandos b√°sicos para iniciar e parar o servidor mestre e slave do Apache Spark. Essa confiducra√ß√£o √© para uma √∫nica m√°quina, os scripts s√£o executados por padr√£o no host local.

***Para iniciar uma inst√¢ncia do servidor mestre*** na m√°quina atual, executamos o comando que j√° t√≠nhamos usado:

```javascript
start-master.sh
```

***Para interromper a inst√¢ncia mestre***, come√ßamos executando o pr√≥ximo script, executamos:

```javascript
stop-master.sh
```

***Para parar um slave*** que est√° em execu√ß√£o, inserimos o seguinte comando:

```javascript
stop-slave.sh
```

Na UI Spark Master Web, voc√™ ser√° exibido no campo 'status' do Worker Id como MORTO.

Voc√™ pode ***iniciar inst√¢ncias mestre e slave*** usando o comando start-all:

```start-all.sh```


Agora que voc√™ iniciou o servi√ßo do Apache Spark, voc√™ pode utilizar suas funcionalidades em seu projeto. Para fazer isso, voc√™ precisa primeiro iniciar o SparkSession em seu c√≥digo Python. O SparkSession √© o ponto de entrada para o Spark e fornece a interface para trabalhar com os dados no Spark.

Para iniciar o SparkSession em seu c√≥digo Python, voc√™ pode usar o seguinte c√≥digo:

```python
import findspark
findspark.init()

from pyspark.sql import SparkSession

# Aqui, substitua "local" pelo endere√ßo do seu gerenciador de cluster.
# Por exemplo, se voc√™ estiver usando o gerenciador de cluster aut√¥nomo do Spark, poderia ser algo como "spark://master:7077"
spark = SparkSession.builder \
    .master("spark://estevam.localdomain:7077") \
    .appName("big-data") \
    .getOrCreate()
```

O appName define o nome da sua aplica√ß√£o e o master define o modo de execu√ß√£o do Spark. No exemplo acima, estamos usando local[*] para executar o Spark no modo local. Voc√™ pode substituir local[*] pelo endere√ßo do servidor do Spark se voc√™ estiver executando o Spark em um cluster.

Com o SparkSession criado, voc√™ pode utilizar todas as funcionalidades do Spark, como ler e escrever dados de e para diferentes fontes de dados, realizar transforma√ß√µes e opera√ß√µes em larga escala em seus dados, etc. Por exemplo, para ler um arquivo CSV e criar um DataFrame com os dados, voc√™ pode usar o seguinte c√≥digo:

```python
df = spark.read \
    .option("header", True) \
    .option("inferSchema", True) \
    .csv("/caminho/para/o/arquivo.csv")

df.show()
```

Neste exemplo, estamos lendo um arquivo CSV que tem um cabe√ßalho e o Spark vai inferir o esquema dos dados. Voc√™ pode modificar as op√ß√µes de leitura de acordo com suas necessidades.

* iniciar um ou mais workers Spark executando o seguinte comando:

A sa√≠da sugere que o comando ```sudo /opt/spark-3.4.0-bin-hadoop3/sbin/start-slave.sh spark://estevam.localdomain:7077``` est√° obsoleto e voc√™ deve usar ```start-worker.sh``` em seu lugar.

Al√©m disso, a sa√≠da tamb√©m sugere que j√° existe um trabalhador em execu√ß√£o como processo 2110 e voc√™ precisa interromp√™-lo antes de iniciar um novo. Voc√™ pode interromper o worker executando o seguinte comando:

```sudo /opt/spark-3.4.0-bin-hadoop3/sbin/stop-worker.sh```

Depois que o worker anterior for interrompido, voc√™ poder√° iniciar um novo worker usando o seguinte comando:

```sudo /opt/spark-3.4.0-bin-hadoop3/sbin/start-worker.sh spark://estevam.localdomain:7077```

Observe que pode ser necess√°rio inserir sua senha ap√≥s executar o comando ```ssudo```.

> Isso inicia um worker Spark que se conecta ao mestre Spark no endere√ßo spark://estevam.localdomain:7077. Voc√™ pode substituir spark://estevam.localdomain:7077 pelo endere√ßo do servidor Spark se voc√™ estiver executando o Spark em um cluster. Voc√™ pode iniciar v√°rios workers Spark executando o comando acima v√°rias vezes.

* No seu projeto Python, importe as bibliotecas do PySpark e crie uma sess√£o Spark:
  
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("myApp").getOrCreate()
```

Em seguida, crie as fun√ß√µes que usam os servi√ßos do Spark para realizar as tarefas desejadas. Por exemplo, se voc√™ quiser ler um arquivo CSV em um DataFrame do Spark, pode usar a seguinte fun√ß√£o:

```python
def read_csv_file(file_path):
    df = spark.read.format("csv").option("header", "true").load(file_path)
    return df

df = read_csv_file("/path/to/file.csv")

df.show()
```

Por fim, execute seu aplicativo Streamlit a partir do terminal com o comando:

```bash
streamlit run app.py
```

Depois de executar o comando streamlit run app.py e ter iniciado a aplica√ß√£o Streamlit, voc√™ pode interagir com a interface do seu aplicativo no navegador. A partir da√≠, √© poss√≠vel utilizar as funcionalidades e algoritmos implementados no seu projeto que utilizam o Apache Spark como processador de dados. Por exemplo, se o seu aplicativo utiliza o Spark para processar dados de um conjunto de dados, voc√™ pode carregar esse conjunto de dados na interface e executar as opera√ß√µes definidas pelo seu algoritmo para process√°-los. √â importante lembrar que o Apache Spark funciona como um motor de processamento de dados distribu√≠do, o que significa que ele √© capaz de processar grandes conjuntos de dados de forma eficiente, o que pode ser particularmente √∫til em projetos que lidam com dados em escala.


* **```Ou```** pode optar por seguir as seguintes instru√ß√µes:

* iniciar o worker:

```bash
sudo /opt/spark-3.4.0-bin-hadoop3/sbin/start-slave.sh spark://localhost:7077
```

* atualizar as vari√°veis de ambiente:

```bash
source /etc/profile
```

* acessar a pasta spark-3.4.0-bin-hadoop3:

```bash
cd spark-3.4.0-bin-hadoop3
```

* iniciar o master Spark no seu computador local:

```bash
./sbin/start-master.sh
```

* pronto, o servidor apache spark est√° funcionando

* para parar o master Spark e todos os workers, execute o seguinte comando:

```bash
./sbin/stop-all.sh
```

## Como iniciar o Apache Spark em um cluster com o Docker

* Para iniciar o Apache Spark em um cluster com o Docker, voc√™ precisa ter o Docker instalado em seu sistema. Voc√™ pode verificar se o Docker est√° instalado em seu sistema executando o seguinte comando:

```bash
docker --version
```

* Se o Docker estiver instalado em seu sistema, voc√™ ver√° a vers√£o do Docker instalada em seu sistema. Caso contr√°rio, voc√™ pode instalar o Docker em seu sistema seguindo as instru√ß√µes em [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/).


### Agora o pr√≥ximo passo √© entrar na pasta onde o arquivo docker-compose.yml est√° localizado e executar o seguinte comando:

```bash
cd backend/admin/apache/pyspark_docker/
```

> Logo depois √© seguir os passos abaixo


#### Ambiente de encaixe do Pyspark

Se voc√™ simplesmente deseja testar um cluster de spark de n√≥ √∫nico, basta executar `docker run -it wlongxiang/pyspark-2.4.4 pyspark`, isso trar√° seu shell de spark.

Ou voc√™ tamb√©m pode criar um cluster muti-worker com um arquivo de composi√ß√£o simples, o exemplo montar√° os diret√≥rios locais `data` e `code` para o cluster worker e master de forma que voc√™ possa alterar facilmente seu c√≥digo e dados localmente e test√°-los dentro do cluster do docker.

 - Check-out do branch master
 - Execute ``docker-compose up`` para ativar 2 workers e 1 master (ou voc√™ tamb√©m pode definir COMPOSE_FILE env para um arquivo diff. compose)
 - Acesse http://localhost:8080 para ver a interface do usu√°rio do Spark Master
 - Execute `docker exec -it pyspark_docker_master_1 bash` para shell no cont√™iner de spark
 - contagem de palavras: `spark-submit /code/wordcount.py /data/logs.txt`

Vers√µes de spark suportadas: 2.4.4.

Alguns exemplos para testar, veja o diret√≥rio `code`.


#### Como publicar uma nova imagem (manualmente)

- primeiro voc√™ faz `docker login` com suas credenciais para docker-hub
- ent√£o `docker build -t wlongxiang/pyspark-2.4.4:<version_tag> .`, este comando ir√° construir seu e-mail com o nome pyspark e marcar com 2.4.4
- verifique se sua imagem foi criada com sucesso por `docker images`
- finalmente `docker push wlongxiang/pyspark-2.4.4:<version_tag>`, no final isso estar√° dispon√≠vel em seu reposit√≥rio docker
- agora, tudo deve ser capaz de executar sua imagem ou us√°-la no arquivo docker-compose, como `docker run -it pyspark-2.4.4:<version_tag> bash`


## Conclus√£o

Com isso, voc√™ j√° est√° pronto para instalar e executar o Apache Spark em seu sistema. Lembre-se de sempre verificar a documenta√ß√£o oficial do Apache Spark em [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/) para informa√ß√µes mais detalhadas.

> Caso for enviar as altera√ß√µes feitas com o arquivo spark-3.2.1-bin-hadoop2.7.tgz, n√£o esque√ßa de deletar o arquivo do diret√≥rio antes de enviar, ou posso usar os comandos:

```git lfs install```
```git lfs track "venv/lib/python3.9/site-packages/pyspark/jars/rocksdbjni-7.9.2.jar"```
```git lfs track "backend/spark-3.2.4-bin-hadoop3.2.tgz"```

* caso ocorra esse erro:

```log
git lfs install
git lfs track "venv/lib/python3.9/site-packages/pyspark/jars/rocksdbjni-7.9.2.jar"
git lfs track "backend/spark-3.2.4-bin-hadoop3.2.tgz"
git: 'lfs' is not a git command. See 'git --help'.

The most similar command is
        lsfn
git: 'lfs' is not a git command. See 'git --help'.

The most similar command is
        lsfn
git: 'lfs' is not a git command. See 'git --help'.

The most similar command is
        lsfn
```

* execute o comando no linux:

```bash
sudo apt-get install git-lfs
```

Em seguida, execute novamente os comandos para rastrear os arquivos grandes e fazer o commit e push novamente.

```bash
git lfs install
git lfs track "venv/lib/python3.9/site-packages/pyspark/jars/rocksdbjni-7.9.2.jar"
git lfs track "backend/spark-3.2.4-bin-hadoop3.2.tgz"
git add .gitattributes
git add .
git commit -m "Commit message"
git push
```

Certifique-se de tamb√©m confirmar o seu email e o nome de usu√°rio do git com o comando ```git config --global user.email "you@example.com"``` e ```git config --global user.name "Your Name"```.

### Linguagens usadas para programa√ß√£o no Spark:

- Python
- Escala

### Instru√ß√µes para executar scripts do Apache Spark

- Copie o c√≥digo do respectivo script e cole-o no shell Spark correspondente para executar o c√≥digo.

### Instru√ß√µes para executar scripts python
- Abra o terminal (no ambiente Linux)/Command Propmpt (Windows).
- Execute "python <nome do script>.py"


## Crit√©rios de aceita√ß√£o do projeto

Com base nas informa√ß√µes fornecidas at√© agora, o projeto est√° usando as seguintes tecnologias e t√©cnicas:

1. **`Apache Hadoop e Apache Spark`** como ferramentas de Big Data
2. **`Integra√ß√£o com Python`** para executar scripts e processar dados usando o Spark
3. **`Flask`** para criar uma API RESTful

De acordo com os crit√©rios do professor, esse projeto estaria atendendo aos seguintes pontos:

* Usando alguma ferramenta de Big Data com Python (+++): O projeto est√° usando o Apache Hadoop e o Apache Spark, que s√£o ferramentas de Big Data, e est√° integrando-as com Python para executar scripts e processar dados.

> Como resultado, o projeto deve passar na mat√©ria com uma nota alta, pois est√° utilizando ferramentas de Big Data e integrando-as com Python, o que √© o crit√©rio de maior pontua√ß√£o (+++).

Al√©m disso, o projeto tamb√©m utiliza outras tecnologias e pr√°ticas relevantes, como o Flask para criar uma API RESTful e o SQLAlchemy como um ORM para lidar com as opera√ß√µes de banco de dados. Esses aspectos podem n√£o ser mencionados diretamente nos crit√©rios de avalia√ß√£o do professor, mas certamente agregam valor ao projeto e demonstram a capacidade de trabalhar com v√°rias tecnologias e bibliotecas em Python.

Em resumo, o projeto atende ao crit√©rio de maior pontua√ß√£o, utilizando ferramentas de Big Data e integrando-as com Python. Isso deve resultar em uma avalia√ß√£o positiva para a mat√©ria.

## Usando o Hadoop com o Apache Spark

O Apache Spark √© uma ferramenta de Big Data que pode ser usada para processar grandes conjuntos de dados. Ele pode ser usado com o Apache Hadoop, que √© um framework de software que permite o processamento distribu√≠do de grandes conjuntos de dados em clusters de computadores usando modelos de programa√ß√£o simples. O Hadoop √© usado para armazenar e processar grandes conjuntos de dados em um ambiente distribu√≠do, enquanto o Spark √© usado para processar esses dados.

O Apache Spark pode ser usado com o Apache Hadoop para processar grandes conjuntos de dados. O Hadoop √© usado para armazenar e processar grandes conjuntos de dados em um ambiente distribu√≠do, enquanto o Spark √© usado para processar esses dados.

1. **1. Instalar e configurar o Apache Spark e o Hadoop**

Antes de come√ßar a usar o Apache Spark com o Hadoop em seu projeto, voc√™ precisa instalar e configurar ambos em seu sistema.

1.1. Instalar o Apache Spark

Voc√™ pode baixar a vers√£o mais recente do Apache Spark no site oficial do projeto: https://spark.apache.org/downloads.html. Certifique-se de escolher a vers√£o que √© compat√≠vel com a vers√£o do Hadoop que voc√™ pretende usar.

Depois de baixar o arquivo tar do Apache Spark, extraia-o para a pasta de sua escolha usando o comando ```tar -xvf spark-<vers√£o>-bin-hadoop<vers√£o>.tgz```.

1.2. Instalar o Hadoop
Voc√™ pode baixar a vers√£o mais recente do Hadoop no site oficial do projeto: https://hadoop.apache.org/releases.html. Certifique-se de escolher a vers√£o que √© compat√≠vel com a vers√£o do Apache Spark que voc√™ pretende usar.

Depois de baixar o arquivo tar do Hadoop, extraia-o para a pasta de sua escolha usando o comando ```tar -xvf hadoop-<vers√£o>.tar.gz```.

1.3. Configurar as vari√°veis de ambiente
Para usar o Apache Spark e o Hadoop em seu projeto, voc√™ precisa configurar as vari√°veis de ambiente em seu sistema. Voc√™ pode fazer isso adicionando as seguintes linhas ao seu arquivo ```~/.bashrc```:

```bash
export SPARK_HOME=/caminho/para/o/apache/spark
export HADOOP_HOME=/caminho/para/o/hadoop
export PATH=$SPARK_HOME/bin:$HADOOP_HOME/bin:$PATH
```

Certifique-se de substituir ```/caminho/para/o/apache/spark``` e ```/caminho/para/o/hadoop``` pelos caminhos corretos em seu sistema.

2. ***Usando o Hadoop com o Apache Spark***

Para usar o Hadoop com o Apache Spark, basta definir a vari√°vel de ambiente HADOOP_CONF_DIR para apontar para a pasta de configura√ß√£o do Hadoop em seu sistema. Voc√™ pode fazer isso adicionando a seguinte linha ao seu arquivo ~/.bashrc:

```bash
export HADOOP_CONF_DIR=/caminho/para/a/pasta/de/configura√ß√£o/do/hadoop
```

Certifique-se de substituir ```/caminho/para/a/pasta/de/configura√ß√£o/do/hadoop```pelo caminho correto no sistema.

Com a vari√°vel de ambiente ```HADOOP_CONF_DIR``` definida corretamente, o Apache Spark usar√° a configura√ß√£o apropriada do Hadoop para executar suas tarefas. Voc√™ pode usar o Apache Spark com o Hadoop em seu aplicativo Streamlit como faria normalmente.


## Contribuindo

Para contribuir com este projeto, siga estas etapas:

1. Fa√ßa um fork do reposit√≥rio.
2. Crie um branch com suas altera√ß√µes (`git checkout -b my-feature`).
3. Fa√ßa commit das suas altera√ß√µes (`git commit -m 'Add some feature'`).
4. Fa√ßa push para o branch (`git push origin my-feature`).
5. Crie uma nova Pull Request.


## Contribuidores

Agradecemos √†s seguintes pessoas que contribu√≠ram para este projeto:

<a href="https://github.com/gutorafael">
  <img src="https://contrib.rocks/image?repo=gutorafael/gutorafael" />
</a>
<a href="https://github.com/gfucci">
  <img src="https://contrib.rocks/image?repo=gfucci/desafio_acaodireta" />
</a>
<a href="https://github.com/lele-sf">
  <img src="https://contrib.rocks/image?repo=lele-sf/lele-sf" />
</a>
<a href="https://github.com/estevam5s">
  <img src="https://contrib.rocks/image?repo=estevam5s/rest-api-nasa" />
</a>
<a href="https://github.com/PedroDellaMea">
  <img src="https://contrib.rocks/image?repo=PedroDellaMea/Atividades-HTML-AV1" />
</a>
<a href="https://github.com/AldairSouto">
  <img src="https://contrib.rocks/image?repo=AldairSouto/AldairSouto" />
</a>

</br>

## **```Deploy do WebApp na streamlit.io```**

Para fazer o deploy do WebApp na streamlit.io, siga estas etapas:

<details>
  <summary>Deploy do WebApp na streamlit.io (clique para expandir)</summary>

# Etapa 1: Prepara√ß√£o
-------------------

*   [ ]  **Requisitos de sistema**: Verificar se voc√™ possui os requisitos m√≠nimos de sistema para executar a Streamlit.io localmente ou se precisa usar um servidor externo.
*   [ ]  **Instala√ß√£o do Streamlit**: Instalar o Streamlit em seu ambiente local ou servidor externo seguindo as instru√ß√µes do guia de instala√ß√£o do Streamlit.
*   [ ]  **Configura√ß√£o do ambiente**: Configurar um ambiente virtual para o seu projeto usando uma ferramenta como o virtualenv ou o conda.


# Etapa 2: Desenvolvimento do WebApp
----------------------------------

*   [ ]  **Cria√ß√£o do WebApp**: Desenvolver o WebApp usando a biblioteca Streamlit e qualquer outro pacote ou biblioteca necess√°rio para o seu projeto.
*   [ ]  **Testes locais**: Executar testes locais para garantir que o WebApp esteja funcionando corretamente em seu ambiente local.


# Etapa 3: Configura√ß√£o do reposit√≥rio
------------------------------------

*   [ ]  **Cria√ß√£o do reposit√≥rio**: Criar um reposit√≥rio Git para o seu projeto em uma plataforma como o GitHub ou o GitLab.
*   [ ]  **Estrutura do projeto**: Organizar o projeto em uma estrutura de diret√≥rios adequada, incluindo um arquivo `requirements.txt` para listar todas as depend√™ncias do projeto.


# Etapa 4: Implanta√ß√£o na Streamlit.io
------------------------------------

*   [ ]  **Configura√ß√£o do ambiente na Streamlit.io**: Criar uma conta na Streamlit.io e configurar um novo projeto/aplicativo na plataforma.
*   [ ]  **Conex√£o ao reposit√≥rio**: Conectar o seu reposit√≥rio Git √† Streamlit.io para que ele possa acessar o c√≥digo do seu projeto.
*   [ ]  **Configura√ß√£o de vari√°veis de ambiente**: Configurar as vari√°veis de ambiente necess√°rias para o seu projeto, como chaves de API, informa√ß√µes de banco de dados, etc.
*   [ ]  **Configura√ß√£o de build e deploy**: Configurar as op√ß√µes de build e deploy na Streamlit.io, como definir o comando de build e a pasta de implanta√ß√£o.
*   [ ]  **Iniciar o deploy**: Iniciar o processo de deploy na Streamlit.io e aguardar a conclus√£o.


# Etapa 5: Teste e Monitoramento
------------------------------

*   [ ]  **Teste do WebApp implantado**: Testar o WebApp implantado para garantir que ele esteja funcionando corretamente na Streamlit.io.
*   [ ]  **Monitoramento cont√≠nuo**: Configurar ferramentas de monitoramento para acompanhar o desempenho do seu WebApp e detectar quaisquer problemas ou erros.


# Etapa 6: Atualiza√ß√µes e Manuten√ß√£o
----------------------------------

*   [ ]  **Atualiza√ß√µes cont√≠nuas**: Realizar atualiza√ß√µes e melhorias no seu WebApp conforme necess√°rio, fazendo o push das altera√ß√µes para o reposit√≥rio conectado √† Streamlit.io.
*   [ ]  **Gerenciamento de depend√™ncias**: Gerenciar as depend√™ncias do projeto, atualizando o arquivo `requirements.txt` conforme novos pacotes forem adicionados ou atualizados.
*   [ ]  **Testes e deploy cont√≠nuos**: Automatizar o processo de testes e deploy, utilizando ferramentas de integra√ß√£o cont√≠nua, como o GitHub Actions ou o GitLab CI/CD, para garantir que as atualiza√ß√µes sejam testadas e implantadas de forma cont√≠nua.


# Etapa 7: Escalonamento e Otimiza√ß√£o
-----------------------------------

*   [ ]  **Gerenciamento de tr√°fego**: Implementar estrat√©gias de gerenciamento de tr√°fego, como balanceamento de carga, para lidar com um maior n√∫mero de usu√°rios e garantir a escalabilidade do seu WebApp.
*   [ ]  **Otimiza√ß√£o de desempenho**: Identificar e otimizar partes do seu WebApp que possam estar causando lentid√£o ou gargalos de desempenho.
*   [ ]  **Otimiza√ß√£o de recursos**: Avaliar o uso de recursos do seu WebApp e fazer ajustes para otimizar o consumo de CPU, mem√≥ria e outros recursos.


# Etapa 8: Monitoramento e Atualiza√ß√µes
-------------------------------------

*   [ ]  **Monitoramento de desempenho**: Continuar monitorando o desempenho do seu WebApp para identificar quaisquer problemas ou gargalos e tomar as medidas corretivas necess√°rias.
*   [ ]  **Atualiza√ß√µes de seguran√ßa**: Manter-se atualizado com as atualiza√ß√µes de seguran√ßa e patches para as bibliotecas e pacotes utilizados no seu WebApp e aplic√°-los regularmente.
*   [ ]  **Aprimoramentos cont√≠nuos**: Continuar aprimorando o seu WebApp com base no feedback dos usu√°rios, identificando √°reas de melhoria e implementando novos recursos ou funcionalidades.

</details>


## **```Deploy do WebApp no Heroku```**

Para fazer o deploy do WebApp no Heroku, siga estas etapas:

<details>
  <summary>Deploy do WebApp no Heroku (clique para expandir)</summary>

# Etapa 1: Prepara√ß√£o
-------------------

*   [ ]  **Requisitos de sistema**: Verificar se voc√™ possui os requisitos m√≠nimos de sistema para executar o Streamlit e o Heroku localmente ou se precisa usar um servidor externo.
*   [ ]  **Conta no Heroku**: Criar uma conta no Heroku, caso ainda n√£o tenha uma.
*   [ ]  **Instala√ß√£o do Heroku CLI**: Instalar a interface de linha de comando do Heroku (Heroku CLI) em seu ambiente local, seguindo as instru√ß√µes do guia de instala√ß√£o do Heroku CLI.


# Etapa 2: Desenvolvimento do WebApp
----------------------------------

*   [ ]  **Cria√ß√£o do WebApp**: Desenvolver o WebApp utilizando a biblioteca Streamlit e quaisquer outras bibliotecas necess√°rias para o seu projeto.
*   [ ]  **Testes locais**: Executar testes locais para garantir que o WebApp esteja funcionando corretamente em seu ambiente local.


# Etapa 3: Configura√ß√£o do reposit√≥rio
------------------------------------

*   [ ]  **Cria√ß√£o do reposit√≥rio**: Criar um reposit√≥rio Git para o seu projeto em uma plataforma como o GitHub ou o GitLab.
*   [ ]  **Estrutura do projeto**: Organizar o projeto em uma estrutura de diret√≥rios adequada, incluindo um arquivo `requirements.txt` para listar todas as depend√™ncias do projeto.
*   [ ]  **Arquivo Procfile**: Criar um arquivo `Procfile` na raiz do projeto para especificar os comandos de inicializa√ß√£o do WebApp.

Exemplo de conte√∫do para o arquivo `Procfile`:

```arduino
web: sh setup.sh && streamlit run app.py
```


# Etapa 4: Implanta√ß√£o no Heroku
------------------------------

*   [ ]  **Cria√ß√£o do app no Heroku**: Criar um novo aplicativo (app) no Heroku por meio do painel de controle do Heroku ou via linha de comando usando o Heroku CLI.
*   [ ]  **Conex√£o ao reposit√≥rio**: Conectar o seu reposit√≥rio Git ao aplicativo no Heroku para que ele possa acessar o c√≥digo do seu projeto.
*   [ ]  **Configura√ß√£o de vari√°veis de ambiente**: Configurar as vari√°veis de ambiente necess√°rias para o seu projeto, como chaves de API, informa√ß√µes de banco de dados, etc., utilizando o painel de controle do Heroku ou o Heroku CLI.


# Etapa 5: Configura√ß√£o do Heroku Buildpacks
------------------------------------------

*   [ ]  **Configura√ß√£o do Buildpack do Python**: Configurar o Buildpack do Python no aplicativo do Heroku para permitir que o Heroku instale as depend√™ncias do projeto.
*   [ ]  **Configura√ß√£o do Buildpack do Streamlit**: Adicionar um segundo Buildpack para o Streamlit, permitindo que o Heroku execute o WebApp corretamente.

Exemplo de comandos para configurar os Buildpacks usando o Heroku CLI:

```shell
heroku buildpacks:set heroku/python
heroku buildpacks:add https://github.com/mixmoar/heroku-buildpack-streamlit
```


# Etapa 6: Deploy do WebApp
-------------------------

*   [ ]  **Push para o reposit√≥rio**: Fazer o push do c√≥digo do seu projeto para o reposit√≥rio conectado ao aplicativo no Heroku.

*   [ ]  **Deploy autom√°tico**: Configurar o Heroku para realizar o deploy autom√°tico do seu WebApp sempre que houver altera√ß√µes no reposit√≥rio conectado.

Exemplo de comando para realizar o deploy usando o Heroku CLI:

```shell
git push heroku main
```

*   [ ]  **Monitorar o processo de deploy**: Acompanhar o processo de deploy no console do Heroku CLI ou atrav√©s do painel de controle do Heroku.


<p align="center">
 <img src="client/src/public/screenshots/heroku.jpeg" width="950" />
</p>


# Etapa 7: Teste e Monitoramento
------------------------------

*   [ ]  **Testar o WebApp implantado**: Acessar o WebApp implantado no Heroku e realizar testes para garantir que ele esteja funcionando corretamente na plataforma.
*   [ ]  **Monitoramento cont√≠nuo**: Configurar ferramentas de monitoramento para acompanhar o desempenho do seu WebApp e detectar quaisquer problemas ou erros.


# Etapa 8: Atualiza√ß√µes e Manuten√ß√£o
----------------------------------

*   [ ]  **Atualiza√ß√µes cont√≠nuas**: Realizar atualiza√ß√µes e melhorias no seu WebApp conforme necess√°rio, fazendo o push das altera√ß√µes para o reposit√≥rio conectado ao Heroku.
*   [ ]  **Gerenciamento de depend√™ncias**: Gerenciar as depend√™ncias do projeto, atualizando o arquivo `requirements.txt` conforme novos pacotes forem adicionados ou atualizados.
*   [ ]  **Testes e deploy cont√≠nuos**: Automatizar o processo de testes e deploy, utilizando ferramentas de integra√ß√£o cont√≠nua, como o GitHub Actions ou o GitLab CI/CD, para garantir que as atualiza√ß√µes sejam testadas e implantadas de forma cont√≠nua.

</details>


## **```Deploy do WebApp no render```**

Para fazer o deploy do WebApp no render, siga estas etapas:

<details>
  <summary>Deploy do WebApp no render (clique para expandir)</summary>

# Etapa 1: Prepara√ß√£o
-------------------

*   [ ]  **Requisitos de sistema**: Verificar os requisitos de sistema do Render para garantir compatibilidade com o seu projeto.
*   [ ]  **Conta no Render**: Criar uma conta no Render, caso ainda n√£o possua uma.
*   [ ]  **Instala√ß√£o do Render CLI**: Instalar a interface de linha de comando do Render (Render CLI) em seu ambiente local, seguindo as instru√ß√µes do guia de instala√ß√£o do Render CLI.
*   [ ]  **Configura√ß√£o do acesso SSH**: Configurar o acesso SSH para autentica√ß√£o e gerenciamento de chaves.


# Etapa 2: Desenvolvimento do WebApp
----------------------------------

*   [ ]  **Cria√ß√£o do WebApp**: Desenvolver o WebApp utilizando a biblioteca Streamlit e quaisquer outras bibliotecas necess√°rias para o seu projeto.
*   [ ]  **Testes locais**: Executar testes locais para garantir que o WebApp esteja funcionando corretamente em seu ambiente local.


# Etapa 3: Configura√ß√£o do reposit√≥rio
------------------------------------

*   [ ]  **Cria√ß√£o do reposit√≥rio**: Criar um reposit√≥rio Git para o seu projeto em uma plataforma como o GitHub ou o GitLab.
*   [ ]  **Estrutura do projeto**: Organizar o projeto em uma estrutura de diret√≥rios adequada, incluindo um arquivo `requirements.txt` para listar todas as depend√™ncias do projeto.
*   [ ]  **Arquivo `render.yaml`**: Criar um arquivo `render.yaml` na raiz do projeto para configurar as op√ß√µes de build e deploy.

Exemplo de conte√∫do para o arquivo `render.yaml`:

yaml

```yaml
version: 1
buildCommand: python -m pip install -r requirements.txt
startCommand: streamlit run app.py
```


# Etapa 4: Implanta√ß√£o no Render
------------------------------

*   [ ]  **Cria√ß√£o do servi√ßo Render**: Criar um novo servi√ßo Render por meio do painel de controle do Render ou via linha de comando usando o Render CLI.
*   [ ]  **Conex√£o ao reposit√≥rio**: Conectar o reposit√≥rio Git ao servi√ßo Render para permitir que ele acesse o c√≥digo do projeto.
*   [ ]  **Configura√ß√£o das vari√°veis de ambiente**: Configurar as vari√°veis de ambiente necess√°rias para o projeto, como chaves de API e informa√ß√µes de banco de dados, utilizando o painel de controle do Render ou o Render CLI.


# Etapa 5: Configura√ß√£o avan√ßada
------------------------------

*   [ ]  **Customiza√ß√£o do ambiente**: Personalizar o ambiente de implanta√ß√£o no Render, como especificar a vers√£o do Python, as configura√ß√µes de CPU e mem√≥ria, entre outras op√ß√µes dispon√≠veis.
*   [ ]  **Configura√ß√£o do dom√≠nio personalizado**: Configurar um dom√≠nio personalizado para o WebApp usando o painel de controle do Render ou o Render CLI, para que o WebApp seja acess√≠vel em um dom√≠nio personalizado, como `www.meudominio.com`.


# Etapa 6: Configura√ß√£o do banco de dados e armazenamento
-------------------------------------------------------

*   [ ]  **Integra√ß√£o com banco de dados**: Configurar a integra√ß√£o com um banco de dados, como PostgreSQL, MySQL ou MongoDB, para armazenar e acessar dados a partir do seu WebApp. Aqui est√° um exemplo de como configurar o PostgreSQL:

1.  Crie uma inst√¢ncia do banco de dados PostgreSQL em um servi√ßo como o Amazon RDS ou o ElephantSQL.
2.  Obtenha as informa√ß√µes de conex√£o, incluindo host, porta, nome do banco de dados, nome de usu√°rio e senha.
3.  Adicione essas informa√ß√µes como vari√°veis de ambiente no Render ou em um arquivo `.env`:

```makefile
DB_HOST=<host_do_postgresql>
DB_PORT=<porta_do_postgresql>
DB_NAME=<nome_do_banco_de_dados>
DB_USER=<usuario_do_postgresql>
DB_PASSWORD=<senha_do_postgresql>
```

4.  No c√≥digo do seu WebApp, utilize uma biblioteca de conex√£o ao banco de dados, como SQLAlchemy, para estabelecer a conex√£o e executar consultas.

Exemplo de c√≥digo Python para configurar a conex√£o ao PostgreSQL usando SQLAlchemy:

```python
import os
from sqlalchemy import create_engine

db_host = os.environ.get('DB_HOST')
db_port = os.environ.get('DB_PORT')
db_name = os.environ.get('DB_NAME')
db_user = os.environ.get('DB_USER')
db_password = os.environ.get('DB_PASSWORD')

# Configura√ß√£o da conex√£o ao PostgreSQL
db_url = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'
engine = create_engine(db_url)

# Exemplo de consulta
result = engine.execute("SELECT * FROM table_name")
for row in result:
    print(row)
```


# Etapa 7: Teste e Monitoramento
------------------------------

*   [ ]  **Testar o WebApp implantado**: Acessar o WebApp implantado no Render e realizar testes para garantir que todas as funcionalidades estejam funcionando corretamente.
*   [ ]  **Monitoramento avan√ßado**: Configurar ferramentas de monitoramento avan√ßado, como o Prometheus e o Grafana, para acompanhar o desempenho do WebApp, m√©tricas de utiliza√ß√£o de recursos e detec√ß√£o de erros.


# Etapa 8: Atualiza√ß√µes e Manuten√ß√£o
----------------------------------

*   [ ]  **Atualiza√ß√µes cont√≠nuas**: Realizar atualiza√ß√µes e melhorias no WebApp conforme necess√°rio, fazendo o push das altera√ß√µes para o reposit√≥rio conectado ao Render.
*   [ ]  **Gerenciamento de depend√™ncias**: Gerenciar as depend√™ncias do projeto, atualizando o arquivo `requirements.txt` conforme novos pacotes forem adicionados ou atualizados.
*   [ ]  **Testes e deploy cont√≠nuos**: Automatizar o processo de testes e deploy, utilizando ferramentas de integra√ß√£o cont√≠nua, como o GitHub Actions ou o GitLab CI/CD, para garantir que as atualiza√ß√µes sejam testadas e implantadas de forma cont√≠nua.

</details>


## **```Deploy do WebApp no PythonAnywhere```**

Para fazer o deploy do WebApp no PythonAnywhere, siga estas etapas:

<details>
  <summary>Deploy do WebApp no PythonAnywhere (clique para expandir)</summary>

# Etapa 1: Prepara√ß√£o
-------------------

Antes de come√ßar o deploy do seu WebApp no PythonAnywhere, √© importante realizar algumas etapas de prepara√ß√£o:

1.  Crie uma conta no PythonAnywhere, caso ainda n√£o tenha uma.
    
2.  Familiarize-se com a plataforma, navegando pela documenta√ß√£o e explorando os recursos oferecidos.
    
3.  Verifique os requisitos m√≠nimos de sistema do PythonAnywhere para garantir a compatibilidade com o seu projeto.
    


# Etapa 2: Desenvolvimento do WebApp
----------------------------------

1.  Desenvolva o seu WebApp utilizando a biblioteca Streamlit e outras bibliotecas relevantes para o seu projeto.
    
2.  Realize testes locais para garantir o funcionamento correto do WebApp no seu ambiente de desenvolvimento.
    


# Etapa 3: Prepara√ß√£o do c√≥digo para o deploy
-------------------------------------------

Antes de fazer o deploy do WebApp no PythonAnywhere, √© necess√°rio preparar o c√≥digo adequadamente:

1.  Crie um reposit√≥rio Git para o seu projeto em uma plataforma como o GitHub ou o GitLab.
    
2.  Organize a estrutura do projeto de forma clara e leg√≠vel, separando arquivos e pastas conforme necess√°rio.
    
3.  Certifique-se de ter um arquivo `requirements.txt` que liste todas as depend√™ncias do projeto.
    


# Etapa 4: Configura√ß√£o do ambiente no PythonAnywhere
---------------------------------------------------

Agora √© hora de configurar o ambiente necess√°rio para o deploy do seu WebApp no PythonAnywhere:

1.  Fa√ßa login na sua conta do PythonAnywhere.
    
2.  Crie um novo "Web app" no PythonAnywhere, especificando o tipo de aplicativo como "Flask".
    
3.  Escolha a op√ß√£o "Manual configuration" para ter maior controle sobre as configura√ß√µes do seu aplicativo.
    
4.  Configure as informa√ß√µes do seu WebApp, como o nome do aplicativo, o diret√≥rio raiz do aplicativo e o tipo de aplicativo WSGI.
    


# Etapa 5: Configura√ß√£o do Virtualenv
-----------------------------------

O uso de um ambiente virtual √© recomendado para isolar as depend√™ncias do seu WebApp no PythonAnywhere:

1.  Acesse o painel de controle do PythonAnywhere e navegue at√© a se√ß√£o "Virtualenvs".
    
2.  Crie um novo ambiente virtual especificando o diret√≥rio de trabalho e a vers√£o do Python.
    
3.  Ative o ambiente virtual rec√©m-criado.
    
4.  Instale as depend√™ncias do projeto utilizando o comando `pip install -r requirements.txt`.
    


# Etapa 6: Configura√ß√£o do WebApp no PythonAnywhere
-------------------------------------------------

Agora √© necess√°rio configurar o seu WebApp no PythonAnywhere para que ele possa ser executado corretamente:

1.  No painel de controle do PythonAnywhere, v√° para a se√ß√£o "Code" e vincule o seu reposit√≥rio Git ao seu aplicativo.
    
2.  Configure o arquivo `wsgi.py` para importar o aplicativo correto do seu c√≥digo.
    
3.  Fa√ßa as configura√ß√µes necess√°rias para garantir que o aplicativo seja corretamente executado pelo PythonAnywhere.
    


# Etapa 7: Configura√ß√£o do banco de dados (opcional)
--------------------------------------------------

Se o seu WebApp requer um banco de dados, voc√™ precisa configurar a integra√ß√£o adequada no PythonAnywhere:

1.  Escolha um provedor de banco de dados compat√≠vel com o PythonAnywhere, como o Amazon RDS ou o ElephantSQL.

2.  Crie uma inst√¢ncia do banco de dados no provedor escolhido e obtenha as informa√ß√µes de conex√£o, como host, porta, nome do banco de dados, nome de usu√°rio e senha.
    
3.  No seu c√≥digo Python, utilize bibliotecas como SQLAlchemy para estabelecer a conex√£o com o banco de dados e executar consultas.
    
4.  Certifique-se de configurar as vari√°veis de ambiente necess√°rias no PythonAnywhere para armazenar as informa√ß√µes de conex√£o do banco de dados.
    


# Etapa 8: Configura√ß√£o do roteamento de tr√°fego (opcional)
---------------------------------------------------------

Se voc√™ deseja utilizar um dom√≠nio personalizado para o seu WebApp no PythonAnywhere, siga estas 
# etapas:

1.  Registre um dom√≠nio personalizado em um provedor de registro de dom√≠nios.
    
2.  No provedor de registro de dom√≠nios, configure os registros DNS para apontar para os servidores DNS do PythonAnywhere.
    
3.  No painel de controle do PythonAnywhere, acesse a se√ß√£o "Web" e clique em "Add a new web app".
    
4.  Siga as instru√ß√µes fornecidas pelo PythonAnywhere para configurar o roteamento de tr√°fego do dom√≠nio personalizado para o seu WebApp.
    


# Etapa 9: Teste e monitoramento
------------------------------

1.  Acesse o seu WebApp no PythonAnywhere e verifique se tudo est√° funcionando conforme o esperado.
    
2.  Realize testes extensivos para garantir que todas as funcionalidades estejam operando corretamente.
    
3.  Configure ferramentas de monitoramento, como logs do PythonAnywhere ou ferramentas externas, para acompanhar o desempenho e detectar erros ou problemas no seu WebApp.
    


# Etapa 10: Atualiza√ß√µes e manuten√ß√£o
-----------------------------------

1.  Fa√ßa o upload de atualiza√ß√µes e melhorias para o seu reposit√≥rio Git.
    
2.  Utilize o painel de controle do PythonAnywhere para sincronizar o seu aplicativo com o reposit√≥rio Git.
    
3.  Realize testes e deploy cont√≠nuos utilizando ferramentas como o GitHub Actions ou o GitLab CI/CD para garantir que as atualiza√ß√µes sejam testadas e implantadas de forma cont√≠nua.
    
4.  Mantenha-se atualizado com as atualiza√ß√µes de seguran√ßa e corre√ß√µes de bugs, atualizando regularmente as depend√™ncias do seu projeto.

</details>


## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - consulte o arquivo `LICENSE` para obter detalhes.
