# Projeto Pedacinho do C√©u

Este projeto foi criado para gerenciar um restaurante chamado Pedacinho do C√©u. O projeto utiliza Big Data, Power BI, Docker e uma API RESTful para coletar, processar, armazenar e visualizar os dados.


## Status

| Topics  | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Build   | [![Build Status](https://img.shields.io/travis/big-data-estacio/data/master)](https://travis-ci.com/github/big-data-estacio/data) [![codecov](https://codecov.io/gh/big-data-estacio/data/branch/master/graph/badge.svg)](https://codecov.io/gh/big-data-estacio/data) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) |
| Version | [![Version](https://img.shields.io/github/v/release/big-data-estacio/data)](https://github.com/big-data-estacio/data/releases) [![Release Notes](https://img.shields.io/badge/release-notes-blue)](https://github.com/big-data-estacio/data/releases)                                                                                                                                         |
| Docs    | [![Docs](https://img.shields.io/static/v1?label=docs&message=wiki&color=blue&logo=github)](https://github.com/big-data-estacio/data/wiki)                                                                                                                                                                                                                                                                                                           |
| Support | [![Gitter](https://badges.gitter.im/big-data-estacio/data.svg)](https://gitter.im/big-data-estacio/data?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![Mail](https://img.shields.io/badge/mail-support-brightgreen)](mailto:your-email@example.com)                                                                                                                                                     |
| Binder  | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/big-data-estacio/data/master?urlpath=lab)      |
| Technology | [![Flask](https://img.shields.io/badge/-Flask-black?style=flat&logo=flask)](https://flask.palletsprojects.com/) [![Hadoop](https://img.shields.io/badge/-Hadoop-black?style=flat&logo=hadoop)](https://hadoop.apache.org/) [![Kafka](https://img.shields.io/badge/-Kafka-black?style=flat&logo=apache-kafka)](https://kafka.apache.org/) [![PostgreSQL](https://img.shields.io/badge/-PostgreSQL-black?style=flat&logo=postgresql)](https://www.postgresql.org/) |

---

1. Coleta de dados: Os scripts na pasta `data_collection/scripts/` s√£o executados para coletar dados de v√°rias fontes externas. Esses dados podem ser armazenados na pasta `data_collection/data_sources/` em formatos como CSV, JSON, XML ou qualquer outro formato de dados necess√°rio.

2. Processamento de dados: Os scripts Apache Spark na pasta `data_processing/spark_jobs/` s√£o executados para processar os dados coletados, realizar transforma√ß√µes e an√°lises necess√°rias, e prepar√°-los para armazenamento no banco de dados.

3. Armazenamento de dados: Os dados processados s√£o armazenados no banco de dados PostgreSQL, que est√° configurado na pasta `data_storage/postgres/`. O arquivo `init.sql` cont√©m instru√ß√µes SQL para criar tabelas e estruturas de dados necess√°rias.

4. An√°lise e visualiza√ß√£o de dados: Com os dados armazenados no banco de dados PostgreSQL, voc√™ pode usar o Power BI para se conectar ao banco de dados, criar visualiza√ß√µes e pain√©is interativos para analisar os dados e obter insights valiosos. O Power BI permite criar relat√≥rios detalhados e personalizados que ajudam na tomada de decis√µes e na compreens√£o de tend√™ncias e padr√µes nos dados.

5. Atualiza√ß√£o e manuten√ß√£o: √Ä medida que novos dados s√£o coletados e processados, o projeto pode ser atualizado e mantido para garantir que as an√°lises e visualiza√ß√µes permane√ßam relevantes e precisas. Isso pode incluir ajustes nos scripts de coleta e processamento de dados, bem como modifica√ß√µes nas visualiza√ß√µes e pain√©is do Power BI

O projeto utiliza conceitos de Big Data e Power BI. Vamos revisar como cada tecnologia √© aplicada no projeto:

1. Big Data: O projeto coleta e processa grandes volumes de dados de v√°rias fontes usando scripts personalizados em Python e Apache Spark. Apache Spark √© uma plataforma de computa√ß√£o em cluster r√°pida e geral que √© projetada para processamento de Big Data escalon√°vel e de alto desempenho. Neste projeto, os dados coletados s√£o processados e transformados pelo Apache Spark, tornando-os adequados para an√°lise e visualiza√ß√£o.

2. Power BI: Para a visualiza√ß√£o de dados e an√°lise dos resultados processados, o projeto utiliza o Power BI. Os arquivos de relat√≥rio do Power BI (.pbix) s√£o armazenados na pasta `data_visualization/power_bi_reports`. Esses arquivos podem ser criados e editados usando o aplicativo Power BI Desktop e, em seguida, publicados no servi√ßo Power BI para compartilhamento e colabora√ß√£o online. A integra√ß√£o com o Power BI permite que voc√™ explore e analise os dados coletados e processados, criando visualiza√ß√µes interativas e relat√≥rios din√¢micos.

Combinando Big Data e Power BI, este projeto oferece uma solu√ß√£o completa para coletar, processar, analisar e visualizar grandes volumes de dados em tempo real, ajudando na tomada de decis√µes informadas e oferecendo insights valiosos para o neg√≥cio do restaurante "Pedacinho do C√©u".


## √çndice

1. [üöÄIntrodu√ß√£o](#introdu√ß√£o)
2. [üîÆArvore de Diretorios](#arvore-de-diretorios)
3. [üåÉArquitetura do projeto](#arquitetura-do-projeto)
4. [üéÜTecnologias Utilizadas](#tecnologias-utilizadas)
5. [üíéPr√©-requisitos](#pr√©-requisitos)
6. [‚ú®Instalando o Projeto](#instalando-o-projeto)
7. [üéâExecutando o Projeto](#executando-o-projeto)
   * [Configurando o ambiente virtual](#configurando-o-ambiente-virtual)
   * [Continuando a instala√ß√£o](#continuando-a-instala√ß√£o)
8. [üëæTestes](#testes)
9. [üîùUtilizando a API](#utilizando-a-api)
10. [üî•Utilizando a API com Insomnia](#utilizando-a-api-com-insomnia)
11. [ü•∂Vers√£o atual](#vers√£o-atual)
12. [üëπColetando Dados](#coletando-dados)
13. [üëªProcessando Dados](#processando-dados)
14. [ü§ñVisualizando os Dados](#visualizando-os-dados)
15. [üëΩFuturas Atualiza√ß√µes](#futuras-atualiza√ß√µes)
16. [üê≥Tecnologias e conceitos utilizados](#tecnologias-e-conceitos-utilizados)
    * [Hadoop](#hadoop)
    * [Kafka](#kafka)
    * [Docker](#docker)
    * [SOLID](#solid)
    * [Padr√µes de commit](#padr√µes-de-commit)
17. [üß†Roadmap v1](#roadmap-v1)
    * [Travis CI](#travis-ci)
    * [Princ√≠pios SOLID](#princ√≠pios-solid)
    * [SQLAlchemy com PostgreSQL ou MySQL](#sqlalchemy-com-postgresql-ou-mysql)
    * [Autentica√ß√£o e autoriza√ß√£o](#autentica√ß√£o-e-autoriza√ß√£o)
    * [Interface de usu√°rio](#interface-de-usu√°rio)
18. [üß†Roadmap v2](#roadmap-v2)
    * [Roadmap v1.1](#roadmap-v1.1)
    * [Roadmap v1.2](#roadmap-v1.2)
    * [Roadmap v1.3](#roadmap-v1.3)
    * [Roadmap v2.0](#roadmap-v2.0)
19. [ü§ñO que √© o Apache Spark?](#o-que-√©-o-apache-spark?)
20. [üéñÔ∏èCrit√©rios de aceita√ß√£o do projeto](#crit√©rios-de-aceita√ß√£o-do-projeto)
21. [üëπContribuindo](#contribuindo)
22. [üëæContribuidores](#contribuidores)
23. [üéâLicen√ßa](#licen√ßa)


## Introdu√ß√£o

Este projeto √© um estudo de caso de Big Data e Power BI. O objetivo √© demonstrar como coletar, processar, analisar e visualizar grandes volumes de dados em tempo real usando ferramentas e tecnologias como Python, Apache Spark, PostgreSQL e Power BI.

O projeto √© baseado em um restaurante chamado "Pedacinho do C√©u". O restaurante est√° localizado em uma cidade tur√≠stica e serve comida tradicional da regi√£o. O restaurante est√° interessado em coletar dados de v√°rias fontes para analisar e obter insights sobre o neg√≥cio. O restaurante tamb√©m deseja criar visualiza√ß√µes e relat√≥rios interativos para ajudar na tomada de decis√µes e na compreens√£o de tend√™ncias e padr√µes nos dados.


## Arvore de Diretorios

Abaixo est√° a estrutura de diret√≥rios do projeto:

```bash
.
‚îú‚îÄ‚îÄ api
‚îÇ   ‚îú‚îÄ‚îÄ controller
‚îÇ   ‚îú‚îÄ‚îÄ model
‚îÇ   ‚îú‚îÄ‚îÄ routes
‚îÇ   ‚îî‚îÄ‚îÄ service
‚îú‚îÄ‚îÄ data_collection
‚îÇ   ‚îú‚îÄ‚îÄ raw_data
‚îÇ   ‚îî‚îÄ‚îÄ scripts
‚îú‚îÄ‚îÄ data_processing
‚îÇ   ‚îú‚îÄ‚îÄ config
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ scripts
‚îú‚îÄ‚îÄ data_visualization
‚îÇ   ‚îî‚îÄ‚îÄ power_bi_reports
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ analysis
‚îÇ   ‚îú‚îÄ‚îÄ config
‚îÇ   ‚îú‚îÄ‚îÄ constants
‚îÇ   ‚îú‚îÄ‚îÄ database
‚îÇ   ‚îî‚îÄ‚îÄ utils
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îú‚îÄ‚îÄ integration
‚îÇ   ‚îî‚îÄ‚îÄ unit
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt

```


## Arquitetura do projeto

A arquitetura do projeto √© dividida em v√°rias partes:

* **`data_collection`**: Coleta e armazenamento dos dados em arquivos.
* **`data_processing`**: Processamento dos dados usando Apache Spark.
* **`data_storage`**: Armazenamento dos dados processados em um banco de dados PostgreSQL.
* **`api`**: API RESTful para acessar e gerenciar os dados armazenados.
* **`data_visualization`**: Visualiza√ß√£o dos dados usando relat√≥rios do Power BI.
* **`docker`**: Configura√ß√£o do Docker para executar os servi√ßos do projeto.
* **`docker-compose`**: Configura√ß√£o do Docker Compose para gerenciar os servi√ßos do projeto.
* **`scripts`**: Scripts de inicializa√ß√£o e gerenciamento do projeto.
* **`docs`**: Documenta√ß√£o do projeto.
* **`LICENSE`**: Licen√ßa do projeto.
* **`README.md`**: Arquivo de leia-me do projeto.
* **`requirements.txt`**: Lista de depend√™ncias do projeto.
* **`docker-compose.yml`**: Arquivo de configura√ß√£o do Docker Compose.


## Tecnologias Utilizadas

Neste projeto "Pedacinho do C√©u", diversas tecnologias s√£o utilizadas para coletar, processar, armazenar e visualizar dados. Abaixo est√° uma lista dessas tecnologias e como elas se encaixam no projeto:

1. **Python**: Linguagem de programa√ß√£o principal para scripts de coleta e processamento de dados.

2. **Apache Spark**: Framework de Big Data utilizado para processar grandes volumes de dados em paralelo e em cluster. Spark √© usado em conjunto com o Python (pyspark) para realizar an√°lises de dados em grande escala.

3. **Docker**: Plataforma para desenvolvimento, deploy e execu√ß√£o de aplica√ß√µes em containers. Docker √© usado para empacotar e gerenciar os servi√ßos e depend√™ncias do projeto, como o PostgreSQL e o Apache Spark.

4. **Docker Compose**: Ferramenta para definir e gerenciar aplica√ß√µes multi-container usando arquivos de configura√ß√£o (docker-compose.yml). √â usado para simplificar o processo de inicializa√ß√£o e gerenciamento de todos os servi√ßos envolvidos no projeto.

5. **PostgreSQL**: Sistema de gerenciamento de banco de dados relacional (RDBMS) utilizado para armazenar e gerenciar dados coletados e processados.

6. **Power BI**: Ferramenta de Business Intelligence (BI) da Microsoft para criar relat√≥rios e visualiza√ß√µes de dados. √â usado para analisar e visualizar os dados coletados e processados pelo projeto.

7. **Flask** (opcional): Microframework Python para desenvolvimento de aplica√ß√µes web. Pode ser usado para criar uma API RESTful que exp√µe os dados processados e armazenados para outras aplica√ß√µes ou servi√ßos.

8. **Pandas** (opcional): Biblioteca Python para manipula√ß√£o e an√°lise de dados. Pode ser usada em conjunto com o Apache Spark para realizar an√°lises e limpeza de dados em pequena escala antes de process√°-los no Spark.


## Pr√©-requisitos

* Python 3.6+
* Apache Spark 3.0+
* PostgreSQL 12+
* Power BI Desktop
* Docker
* Docker Compose
* Node.js e npm
* Git
* Insomnia
* IDE de sua prefer√™ncia (PyCharm, VS Code, etc.)
* Terminal de sua prefer√™ncia (Git Bash, PowerShell, etc.)
* Sistema operacional Linux, macOS ou Windows
* Conhecimentos b√°sicos de Python e SQL
* Conhecimentos b√°sicos de Apache Spark e Power BI


## Arquivo de configura√ß√£o package.json

O arquivo `package.json` cont√©m as depend√™ncias do projeto. Para instalar as depend√™ncias, execute o seguinte comando:

```bash
{
  "name": "restaurante",
  "version": "1.0.0",
  "description": "Projeto de restaurante utilizando Python e Flask",
  "main": "api/app.py",
  "scripts": {
    "docker-start": "docker-compose up --build -d",
    "docker-stop": "docker-compose down",
    "build": "docker build -t nome-da-imagem .",
    "lint": "flake8 src/ tests/",
    "start": "make",
    "dev": "make",
    "test": "make test",
    "dev:server": "nodemon api/app.py",
    "test-coverage": "make test-coverage",
    "pre-commit": "pre-commit install",
    "deploy": "./deploy.sh"
  },
  "keywords": [
    "restaurante",
    "python",
    "flask"
  ],
  "author": "grupo-estacio",
  "license": "MIT",
  "devDependencies": {
    "nodemon": "^2.0.22"
  },
  "dependencies": {
    "Flask": "^2.0.0",
    "Flask-Cors": "^3.0.10",
    "Flask-JWT-Extended": "^4.2.3",
    "Flask-Mail": "^0.9.1",
    "Flask-RESTful": "^0.3.9",
    "Flask-SQLAlchemy": "^3.0.0",
    "PyMySQL": "^1.0.2"
  }
}
```


## Instalando o Projeto

1. Clone o reposit√≥rio:

```bash
git clone https://github.com/your_username/pedacinho_do_ceu.git
cd pedacinho_do_ceu
```


## Executando o Projeto

### Configurando o ambiente virtual

√â recomendado utilizar um ambiente virtual para isolar as depend√™ncias do projeto. Siga os passos abaixo para configurar e ativar o ambiente virtual usando o `venv`:

1. Instale o m√≥dulo `venv`, caso ainda n√£o tenha, com o seguinte comando:

```bash
python3 -m pip install --user virtualenv
```

2. Navegue at√© a pasta do projeto e crie um ambiente virtual:

```bash
python3 -m venv venv

# ou, se voc√™ estiver no Windows

py -m venv venv
```

3. Ative o ambiente virtual:

* No Windows:

```bash
.\venv\Scripts\activate

# ou, se voc√™ estiver usando o Git Bash

source venv/Scripts/activate
```

* No macOS e Linux:

```bash
source venv/bin/activate
```

4. Ap√≥s a ativa√ß√£o do ambiente virtual, seu terminal deve mostrar o prefixo `(venv)`.

Agora voc√™ pode executar o projeto com as depend√™ncias instaladas no ambiente virtual. Lembre-se de ativar o ambiente virtual sempre que for trabalhar no projeto.

5. Instale todas as bibliotecas necess√°rias usando o comando `pip install`:

```bash
pip install biblioteca1 biblioteca2 biblioteca3
```

Substitua `biblioteca1`, `biblioteca2` e `biblioteca3` pelos nomes das bibliotecas que voc√™ deseja instalar.

6. Agora que todas as bibliotecas est√£o instaladas, execute o seguinte comando para gerar o arquivo `requirements.txt`:

```bash
pip freeze > requirements.txt
```

O comando `pip freeze` listar√° todas as bibliotecas instaladas no ambiente virtual e suas vers√µes espec√≠ficas. O operador `>` redirecionar√° a sa√≠da para o arquivo `requirements.txt`, criando-o ou atualizando-o, se j√° existir.

7. Instale as depend√™ncias do projeto utilizando o arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

> arquivo requirements.txt para esse projeto com as depend√™ncias necess√°rias.

```bash
alembic==1.10.3
aniso8601==9.0.1
anyio==3.6.2
arabic-reshaper==3.0.0
asgiref==3.6.0
astroid==2.15.2
attrs==22.2.0
bcrypt==4.0.1
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764672617/work
certifi==2022.12.7
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179360775/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work
click==8.1.3
colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work
conda==23.1.0
conda-package-handling @ file:///home/conda/feedstock_root/build_artifacts/conda-package-handling_1669907009957/work
conda_package_streaming @ file:///home/conda/feedstock_root/build_artifacts/conda-package-streaming_1669733752472/work
contourpy==1.0.7
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1677821254559/work
cycler==0.11.0
dill==0.3.6
distlib==0.3.6
Django==3.2.2
django-crispy-forms==1.11.2
django-extensions==3.1.3
django-widget-tweaks==1.4.8
docopt==0.6.2
exceptiongroup==1.1.1
fastapi==0.94.1
filelock==3.10.6
flasgger==0.9.5
Flask==2.2.3
Flask-Bcrypt==1.0.1
Flask-Cors==3.0.10
Flask-JWT-Extended==4.4.4
Flask-Migrate==4.0.4
Flask-RESTful==0.3.9
Flask-SQLAlchemy==2.4.4
Flask-Testing==0.8.1
fonttools==4.39.2
greenlet==2.0.2
gunicorn==20.1.0
hdfs==2.7.0
html5lib==1.1
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.0.0
importlib-resources==5.12.0
iniconfig==2.0.0
init==0.1.0
isort==5.12.0
itsdangerous==2.1.2
Jinja2==3.1.2
jsonschema==4.17.3
kafka==1.3.5
kiwisolver==1.4.4
lazy-object-proxy==1.9.0
Mako==1.2.4
MarkupSafe==2.1.2
matplotlib==3.7.1
mccabe==0.7.0
mistune==2.0.5
mysql-connector==2.2.9
numpy==1.24.2
packaging==23.0
pandas==1.3.4
Pillow==8.2.0
platformdirs==3.2.0
pluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1667232663820/work
psycopg2==2.9.6
pycosat @ file:///home/conda/feedstock_root/build_artifacts/pycosat_1666836642684/work
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydantic==1.10.6
PyJWT==2.6.0
pylint==2.17.2
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1672659226110/work
pyparsing==3.0.9
PyPDF2==3.0.1
pyrsistent==0.19.3
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pytest==7.2.2
python-bidi==0.4.2
python-dateutil==2.8.2
python-dotenv==0.15.0
pytz==2022.7.1
PyYAML==6.0
reportlab==3.6.9
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1673863902341/work
ruamel.yaml @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml_1678272973380/work
ruamel.yaml.clib @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml.clib_1670412733608/work
six==1.16.0
sniffio==1.3.0
SQLAlchemy==1.3.23
sqlparse==0.4.3
starlette==0.26.1
terminal==0.4.0
tomli==2.0.1
tomlkit==0.11.7
toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work
tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1677948868469/work
ttkthemes==3.2.2
typing_extensions==4.5.0
urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work
virtualenv==20.21.0
webencodings==0.5.1
Werkzeug==2.2.3
wrapt==1.15.0
xhtml2pdf==0.2.5
zipp==3.15.0
zstandard==0.19.0
```

8. Quando terminar de trabalhar no projeto, voc√™ pode desativar o ambiente virtual com o seguinte comando:

```bash
deactivate
```

### üì¶ Continuando a instala√ß√£o

1. Crie um arquivo `.env` na raiz do projeto com as vari√°veis de ambiente necess√°rias. Voc√™ pode usar o arquivo `.env.example` como modelo.

2. Instale as depend√™ncias do projeto:

```bash
npm install
```

### Para iniciar o projeto "Pedacinho do C√©u" com o Docker, siga estas etapas:

1. Certifique-se de que o Docker e o Docker Compose estejam instalados em seu sistema. Se voc√™ ainda n√£o os instalou, siga as instru√ß√µes de instala√ß√£o no site oficial do Docker: `https://docs.docker.com/get-docker/` e `https://docs.docker.com/compose/install/`

2. Abra um terminal e navegue at√© o diret√≥rio raiz do projeto `pedacinho_do_ceu`.

3. Execute o seguinte comando para criar e iniciar os containers do projeto, conforme definido no arquivo `docker-compose.yml`:

```bash
docker-compose up -d
```

Isso iniciar√° os servi√ßos do Docker, a API, Hadoop, como o PostgreSQL, o Apache Spark e o servi√ßo de processamento de dados se necess√°rio.

4. Ap√≥s a inicializa√ß√£o dos servi√ßos, voc√™ pode executar os scripts de coleta de dados na pasta `data_collection/scripts/` para coletar e armazenar os dados no banco de dados PostgreSQL.

5. Em seguida, execute os jobs do Apache Spark na pasta `data_processing/spark_jobs/` para processar os dados coletados e armazenados.

6. Abra os relat√≥rios do Power BI na pasta `data_visualization/power_bi_reports/` para visualizar e analisar os dados processados. Se voc√™ n√£o possui o Power BI Desktop, fa√ßa o download e instale-o a partir do site oficial: `https://powerbi.microsoft.com/en-us/desktop/`

7. Ao longo do desenvolvimento do projeto, voc√™ pode modificar e ajustar os arquivos na pasta `src/` conforme necess√°rio para aprimorar a an√°lise e o processamento dos dados.

Lembre-se de que, dependendo da configura√ß√£o do projeto, algumas etapas podem variar ou exigir a√ß√µes adicionais. Ajuste as etapas conforme necess√°rio para se adequar √†s necessidades espec√≠ficas do seu projeto.

8. Quando terminar de usar o projeto, pare os servi√ßos do Docker Compose pressionando `Ctrl+C` no terminal onde o Docker Compose est√° sendo executado. Para remover os cont√™ineres e os volumes, execute:

```bash
docker-compose down --remove-orphans --volumes
```

9. Inicie o servidor da API:
  
```bash
npm start
```

> J√° o arquivo api/app.py, √© o arquivo principal da API, onde √© feita a conex√£o com o banco de dados e a defini√ß√£o dos endpoints.

Ele √© executado na porta 5000, e pode ser acessado em `http://localhost:5000/`

Nesse arquivo, se encontram os endpoints da API, que s√£o:

* **`GET /`**: Lista todos os itens armazenados no banco de dados.
* **`GET /<id>`**: Retorna os detalhes de um item espec√≠fico com base no ID.
* **`POST /`**: Adiciona um novo item ao banco de dados.
* **`PUT /<id>`**: Atualiza os detalhes de um item espec√≠fico com base no ID.
* **`DELETE /<id>`**: Remove um item espec√≠fico com base no ID.

> @TODO: Adicionar mais detalhes sobre a API
> @Note: A API foi desenvolvida com o framework FastAPI, que √© um framework web ass√≠ncrono de alto desempenho, f√°cil de aprender, r√°pido para codificar, pronto para produ√ß√£o. Ele √© constru√≠do com base no Starlette e Pydantic.

o arquivo `api/app.py` est√° configurado com os scripts de coleta de dados e processamento de dados, para que os dados sejam coletados e processados automaticamente quando a API for iniciada:

```python
import sys
from pathlib import Path
from flask import Flask
from routes.restaurant_routes import restaurant_routes


sys.path.insert(0, str(Path(__file__).resolve().parent))

app = Flask(__name__)

app.register_blueprint(restaurant_routes)

if __name__ == '__main__':
    app.run(debug=True)
```

A API estar√° dispon√≠vel na porta 5000.

9. Abra o Power BI Desktop e carregue os relat√≥rios na pasta `data_visualization/power_bi_reports`.


## Testes

Execute os testes unit√°rios e de integra√ß√£o usando o seguinte comando:

```bash
python -m unittest discover tests
```


## Debugging e Logging

Para depurar o projeto, voc√™ pode usar o depurador interativo do Python. Para isso, basta adicionar o seguinte c√≥digo ao arquivo que voc√™ deseja depurar:

```python
import pdb; pdb.set_trace()
```

Em seguida, execute o arquivo normalmente e o depurador interativo ser√° iniciado quando o c√≥digo atingir o ponto de interrup√ß√£o.

Para adicionar logs ao projeto, voc√™ pode usar o m√≥dulo `logging` do Python. Para isso, basta adicionar o seguinte c√≥digo ao arquivo que voc√™ deseja depurar:

```python
import logging

logging.basicConfig(level=logging.DEBUG)
```

> como que fica o arquivo de log logs/app.log

```log
2023-04-25 00:31:31,719 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:33:49,546 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:34:44,584 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:34:46,857 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 00:34:46,859 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 00:34:46,860 - INFO - Probing node bootstrap-0 broker version
2023-04-25 00:34:47,912 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 00:34:47,915 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 00:36:12,026 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:37:32,931 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:38:27,387 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:38:29,541 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 09:38:29,542 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 09:38:29,542 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:38:30,582 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 09:38:30,582 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 09:39:43,938 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:39:44,780 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:39:44,780 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:39:44,781 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:39:44,782 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:39:44,827 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:39:44,828 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:39:44,828 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:41:52,885 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:41:53,647 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:41:53,648 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:41:53,649 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:41:53,649 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:41:53,693 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:41:53,694 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:41:53,694 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:42:12,859 - INFO - A aplica√ß√£o foi encerrada com sucesso.
```

> e o arquivo de debug:

[![debug](docs/debug.png)](docs/debug.png)

## Utilizando a API

A API RESTful permite gerenciar os dados armazenados no banco de dados PostgreSQL. Para utilizar a API, voc√™ pode fazer requisi√ß√µes HTTP para os seguintes endpoints:

* **`GET /`**: Lista todos os itens armazenados no banco de dados.
* **`GET /<id>`**: Retorna os detalhes de um item espec√≠fico com base no ID.
* **`POST /`**: Adiciona um novo item ao banco de dados.
* **`PUT /<id>`**: Atualiza os detalhes de um item espec√≠fico com base no ID.
* **`DELETE /<id>`**: Remove um item espec√≠fico com base no ID.


## Utilizando a API com Insomnia

Para testar e interagir com a API, voc√™ pode usar o Insomnia REST Client, que √© uma aplica√ß√£o de desktop para gerenciar requisi√ß√µes HTTP e API.

1. Instale o Insomnia em seu computador: Insomnia Download
2. Abra o Insomnia e crie um novo workspace ou use um existente.
3. Para adicionar as rotas da API ao seu workspace, clique em `Create Request`. Escolha o m√©todo HTTP (GET, POST, PUT, DELETE) e insira a URL da API (por exemplo, `http://localhost:5000/` para listar todos os itens). Adicione os par√¢metros, cabe√ßalhos ou corpo da requisi√ß√£o conforme necess√°rio e clique em `Send` para executar a requisi√ß√£o.
4. Voc√™ pode criar v√°rias requisi√ß√µes para diferentes endpoints da API e organiz√°-las em pastas para facilitar a navega√ß√£o.

Agora voc√™ pode usar o Insomnia para testar e explorar a API RESTful do projeto Pedacinho do C√©u.


## Vers√£o atual
A vers√£o atual do projeto inclui as seguintes funcionalidades:

* Coleta e armazenamento de dados em tempo real com Hadoop e Kafka.
* API RESTful para interagir com os dados e servi√ßos.
* Docker Compose para facilitar a instala√ß√£o e configura√ß√£o do projeto.


## Coletando Dados

Os dados podem ser coletados usando o script `data_collection/data_collector.py`. Para coletar os dados, voc√™ pode executar o seguinte comando:

```bash
python data_collection/data_collector.py
```

Os dados coletados ser√£o armazenados em arquivos no diret√≥rio `data_collection/data`.


## Processando Dados

Os dados podem ser processados usando o script `data_processing/data_processor.py`. Para processar os dados, voc√™ pode executar o seguinte comando:

```bash
python data_processing/data_processor.py
```

Os dados processados ser√£o armazenados no banco de dados PostgreSQL.


## Visualizando os Dados

Os dados podem ser visualizados usando relat√≥rios do Power BI. Para visualizar os relat√≥rios, voc√™ pode fazer o download do Power BI Desktop [aqui](https://powerbi.microsoft.com/pt-br/desktop/). Ap√≥s instalar o Power BI Desktop, voc√™ pode abrir o arquivo `pedacinho_do_ceu.pbix` para visualizar os relat√≥rios.


## Futuras Atualiza√ß√µes

* Melhoria na coleta de dados
* Implementa√ß√£o de novas an√°lises e visualiza√ß√µes
* Adi√ß√£o de mais testes
* Melhoria na documenta√ß√£o

O projeto "Pedacinho do C√©u" est√° em constante evolu√ß√£o e melhorias. Abaixo est√£o algumas das atualiza√ß√µes futuras planejadas para o projeto:

1. **Expans√£o das fontes de dados**: Atualmente, o projeto utiliza um conjunto limitado de fontes de dados. No futuro, pretendemos expandir a lista de fontes de dados para enriquecer ainda mais nossas an√°lises e visualiza√ß√µes.

2. **Otimiza√ß√£o do processamento de Big Data**: Continuaremos a melhorar a efici√™ncia e a velocidade do processamento de Big Data usando t√©cnicas de otimiza√ß√£o avan√ßadas e atualiza√ß√µes nas ferramentas e tecnologias utilizadas.

3. **Visualiza√ß√µes interativas**: Atualmente, o projeto utiliza relat√≥rios est√°ticos do Power BI. Planejamos adicionar visualiza√ß√µes interativas e em tempo real para tornar os dados mais acess√≠veis e f√°ceis de entender.

4. **Machine Learning**: Exploraremos a implementa√ß√£o de algoritmos de aprendizado de m√°quina para identificar padr√µes e tend√™ncias nos dados e fornecer insights adicionais.

5. **Integra√ß√£o com outras plataformas**: Planejamos adicionar suporte para integra√ß√£o com outras plataformas de visualiza√ß√£o de dados e an√°lise, como Tableau e Looker.

6. **APIs e microservi√ßos**: Atualmente, o projeto possui uma API simples para acessar os dados. No futuro, pretendemos expandir a API e usar microservi√ßos para fornecer uma solu√ß√£o mais modular e escalon√°vel.

7. **Seguran√ßa e privacidade**: Continuaremos a aprimorar a seguran√ßa e a privacidade dos dados, garantindo a conformidade com os regulamentos de prote√ß√£o de dados e as melhores pr√°ticas do setor.

8. **Documenta√ß√£o e tutoriais**: A documenta√ß√£o e os tutoriais ser√£o atualizados regularmente para garantir que sejam claros, abrangentes e atualizados com as √∫ltimas informa√ß√µes do projeto.


## Tecnologias e conceitos utilizados

### Hadoop
O projeto utiliza o Hadoop e o HDFS (Hadoop Distributed FileSystem) para armazenar e processar grandes volumes de dados.

### Kafka
O projeto utiliza o Apache Kafka para construir sistemas de processamento de dados em tempo real, permitindo lidar com fluxos de dados cont√≠nuos.

### Docker
O projeto √© executado em cont√™ineres Docker para facilitar a implanta√ß√£o e a escalabilidade.

### SOLID
Os princ√≠pios SOLID foram aplicados neste projeto para garantir um c√≥digo limpo, modular e f√°cil de manter.


## Padr√µes de commit
Ao fazer um commit, siga as diretrizes de mensagens de commit personalizadas e com emojis, utilizando at√© 200 caracteres.


## Roadmap v1

* Adicionar suporte ao Travis CI para integra√ß√£o cont√≠nua.
* Aplicar os princ√≠pios SOLID ao projeto para melhorar a manutenibilidade e escalabilidade.
* Atualizar o SQLAlchemy para usar um banco de dados mais robusto, como PostgreSQL ou MySQL, em vez de SQLite.
* Implementar autentica√ß√£o e autoriza√ß√£o para proteger as rotas da API.
* Adicionar testes automatizados para garantir a qualidade do c√≥digo.
* Implementar uma interface de usu√°rio para interagir com a API.

### Travis CI

1. Para adicionar suporte ao Travis CI, siga estas etapas:
2. V√° para `travis-ci.com` e fa√ßa login com sua conta do GitHub.
3. Ative o reposit√≥rio do projeto nas configura√ß√µes do Travis CI.
4. Crie um arquivo `.travis.yml` na raiz do projeto com o seguinte conte√∫do:

```yml
language: python
python:
  - "3.8"

install:
  - pip install -r api/requirements.txt

script:
  - pytest
```

5. Vincule seu reposit√≥rio do GitHub ao Travis CI e habilite a integra√ß√£o cont√≠nua para o projeto.


### Princ√≠pios SOLID
Para aplicar os princ√≠pios SOLID ao projeto, siga estas diretrizes:

1. Single Responsibility Principle (SRP): Separe as responsabilidades do c√≥digo em classes e fun√ß√µes distintas. Por exemplo, separe a l√≥gica de neg√≥cio da l√≥gica de banco de dados.

2. Open/Closed Principle (OCP): Crie classes e fun√ß√µes extens√≠veis sem modificar seu c√≥digo-fonte. Por exemplo, use a heran√ßa e a composi√ß√£o para estender a funcionalidade de uma classe.

3. Liskov Substitution Principle (LSP): As subclasses devem ser substitu√≠veis por suas classes base sem alterar a corre√ß√£o do programa. Por exemplo, verifique se as subclasses seguem o contrato estabelecido pelas classes base.

4. Interface Segregation Principle (ISP): Divida as interfaces grandes em interfaces menores e mais espec√≠ficas. Por exemplo, crie interfaces separadas para opera√ß√µes de leitura e grava√ß√£o em vez de uma interface √∫nica.

5. Dependency Inversion Principle (DIP): Depend√™ncias de alto n√≠vel n√£o devem depender de depend√™ncias de baixo n√≠vel; em vez disso, ambas devem depender de abstra√ß√µes. Por exemplo, use a inje√ß√£o de depend√™ncia para inverter as depend√™ncias entre os m√≥dulos.

### SQLAlchemy com PostgreSQL ou MySQL
Para usar o PostgreSQL ou MySQL com SQLAlchemy, siga estas etapas:

1. Instale os pacotes necess√°rios:

```bash
pip install psycopg2 sqlalchemy psycopg2-binary
```

OU

```bash
pip install pymysql sqlalchemy
```

2. Atualize a string de conex√£o do banco de dados no arquivo api/database.py para usar PostgreSQL ou MySQL:

Para PostgreSQL:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "postgresql://user:password@localhost/dbname"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

Para MySQL:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "mysql+pymysql://user:password@localhost/dbname"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

3. Atualize as configura√ß√µes do banco de dados em `api/__init__.py` para usar a nova string de conex√£o.

### Autentica√ß√£o e autoriza√ß√£o
Para implementar a autentica√ß√£o e autoriza√ß√£o, voc√™ pode usar a extens√£o Flask-Login ou Flask-Security. Siga a documenta√ß√£o oficial de cada extens√£o para configurar a autentica√ß√£o e autoriza√ß√£o no projeto.

### Testes automatizados
Para adicionar testes automatizados, siga estas etapas:

1. Instale o pacote pytest:
  
```bash
pip install pytest
```

2. Crie um diret√≥rio `tests` na raiz do projeto.

3. Dentro do diret√≥rio `tests`, crie arquivos de teste para cada m√≥dulo do projeto. Por exemplo, voc√™ pode criar um arquivo `test_restaurant_service.py` para testar a classe RestaurantService.

4. Escreva testes usando o pytest para cada fun√ß√£o e classe do projeto.

5. Execute os testes usando o comando `pytest` no terminal.

### Interface de usu√°rio
Para implementar uma interface de usu√°rio, voc√™ pode criar uma aplica√ß√£o web usando HTML, CSS e JavaScript para interagir com a API. Voc√™ pode usar bibliotecas como React, Angular ou Vue.js para criar a interface do usu√°rio.


## Roadmap v2

### Vers√£o 1.1
* Implementar autentica√ß√£o e autoriza√ß√£o na API.
* Adicionar suporte para diferentes formatos de dados (JSON, XML, etc.).
* Desenvolver uma interface gr√°fica para visualizar os dados e interagir com a API.

### Vers√£o 1.2
* Adicionar suporte para processamento de dados em tempo real usando Apache Spark ou Flink.
* Implementar algoritmos de machine learning para an√°lise de dados e gera√ß√£o de insights.
* Integrar com outras fontes de dados e APIs de terceiros.

### Vers√£o 1.3
* Adicionar suporte para escalabilidade horizontal e balanceamento de carga.
* Implementar monitoramento e alertas para garantir a sa√∫de e o desempenho do sistema.
* Desenvolver testes de carga e estresse para garantir a estabilidade do projeto em ambientes de produ√ß√£o.

### Vers√£o 2.0
* Redesenhar a arquitetura do projeto para melhorar a modularidade e a manutenibilidade.
* Adicionar suporte para outras linguagens de programa√ß√£o e frameworks.
* Implementar uma API GraphQL para maior flexibilidade na consulta e manipula√ß√£o de dados.


## O que √© o Apache Spark?

![](docs/spark-streaming-datanami.png)
# Apache_Spark
:sparkles::tada: Este reposit√≥rio explica sobre o Apache Spark com pr√°ticas :tada::sparkles:

- √â uma ferramenta de processamento em tempo real desenvolvida para resolver o problema de trabalhar com dados em tempo real.
- √â tratado pelo Apache.
- √â muito r√°pido, d√° resultado no clique.
- Usa avalia√ß√£o pregui√ßosa, ou seja, processa sempre que necess√°rio.
- Como o MapReduce n√£o conseguiu lidar com dados em tempo real, o Spark entrou em cena para ajudar.
- Agora √© usado por muitos grandes gigantes da tecnologia como Oracle, Amazon, Microsoft, Visa, Cisco, Verizon, Hortonworks.
- Como acima, temos 3.000 empresas usando o Apache Spark.

<div align="center">
  <a href="https://www.youtube.com/watch?v=4TE6AGQ0IzI" target="_blank"
  alt="IMAGE ALT TEXT" width="20" height="10" border="10"><img src="https://i.ytimg.com/vi/4TE6AGQ0IzI/maxresdefault.jpg"></a>
</div>

## Baixando o Apache Spark

- Para baixar o Apache Spark, acesse: [Download Apache Spark](https://spark.apache.org/downloads.html)

### Instalando o Apache Spark
- Para Mac: [Instala√ß√£o do Apache Spark](https://github.com/HarshitDawar55/Apache_Spark/blob/master/docs/Installation_Of_Spark_In_Mac.docx)

### Linguagens usadas para programa√ß√£o no Spark:

- Python
- Escala

### Instru√ß√µes para executar scripts do Apache Spark

- Copie o c√≥digo do respectivo script e cole-o no shell Spark correspondente para executar o c√≥digo.

### Instru√ß√µes para executar scripts python
- Abra o terminal (no ambiente Linux)/Command Propmpt (Windows).
- Execute "python <nome do script>.py"


## Crit√©rios de aceita√ß√£o do projeto

Com base nas informa√ß√µes fornecidas at√© agora, o projeto est√° usando as seguintes tecnologias e t√©cnicas:

1. **`Apache Hadoop e Apache Spark`** como ferramentas de Big Data
2. **`Integra√ß√£o com Python`** para executar scripts e processar dados usando o Spark
3. **`Flask`** para criar uma API RESTful

De acordo com os crit√©rios do professor, esse projeto estaria atendendo aos seguintes pontos:

* Usando alguma ferramenta de Big Data com Python (+++): O projeto est√° usando o Apache Hadoop e o Apache Spark, que s√£o ferramentas de Big Data, e est√° integrando-as com Python para executar scripts e processar dados.

> Como resultado, o projeto deve passar na mat√©ria com uma nota alta, pois est√° utilizando ferramentas de Big Data e integrando-as com Python, o que √© o crit√©rio de maior pontua√ß√£o (+++).

Al√©m disso, o projeto tamb√©m utiliza outras tecnologias e pr√°ticas relevantes, como o Flask para criar uma API RESTful e o SQLAlchemy como um ORM para lidar com as opera√ß√µes de banco de dados. Esses aspectos podem n√£o ser mencionados diretamente nos crit√©rios de avalia√ß√£o do professor, mas certamente agregam valor ao projeto e demonstram a capacidade de trabalhar com v√°rias tecnologias e bibliotecas em Python.

Em resumo, o projeto atende ao crit√©rio de maior pontua√ß√£o, utilizando ferramentas de Big Data e integrando-as com Python. Isso deve resultar em uma avalia√ß√£o positiva para a mat√©ria.

## Contribuindo

Para contribuir com este projeto, siga estas etapas:

1. Fa√ßa um fork do reposit√≥rio.
2. Crie um branch com suas altera√ß√µes (`git checkout -b my-feature`).
3. Fa√ßa commit das suas altera√ß√µes (`git commit -m 'Add some feature'`).
4. Fa√ßa push para o branch (`git push origin my-feature`).
5. Crie uma nova Pull Request.


## Contribuidores

Agradecemos √†s seguintes pessoas que contribu√≠ram para este projeto:

<a href="">
  <img src="https://contrib.rocks/image?repo=gutorafael/gutorafael" />
</a>
<a href="">
  <img src="https://contrib.rocks/image?repo=gfucci/desafio_acaodireta" />
</a>
<a href="">
  <img src="https://contrib.rocks/image?repo=lele-sf/lele-sf" />
</a>
<a href="">
  <img src="https://contrib.rocks/image?repo=estevam5s/rest-api-nasa" />
</a>
<a href="">
  <img src="https://contrib.rocks/image?repo=PedroDellaMea/Atividades-HTML-AV1" />
</a>
<a href="">
  <img src="https://contrib.rocks/image?repo=AldairSouto/AldairSouto" />
</a>


## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - consulte o arquivo `LICENSE` para obter detalhes.