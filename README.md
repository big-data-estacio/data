# üî•Painel Streamlit para visualizar, analisar e prever dados de vendas de um restaurante

Este painel foi desenvolvido usando Streamlit. V√°rios pacotes de c√≥digo aberto s√£o usados para processar os dados e gerar as visualiza√ß√µes, por exemplo. [pandas](https://pandas.pydata.org/), [geopandas](https://geopandas.org), [leafmap](https://leafmap.org), [matplotlib](https://matplotlib.org/) e [pydeck](https://deckgl.readthedocs.io).

<p align="center">
 <img src="client/src/public/streamlit.png" width="350" />
</p>


# Projeto Pedacinho do C√©u

Este √© um projeto de an√°lise de dados para um bar e restaurante localizado no sul da ilha de Florian√≥polis. O objetivo do projeto √© utilizar as informa√ß√µes dispon√≠veis para realizar an√°lises e obter insights que possam ser utilizados para tomar decis√µes estrat√©gicas.

O projeto utiliza a linguagem Python e diversas bibliotecas para an√°lise de dados, como Pandas, NumPy e Plotly. Al√©m disso, √© utilizado o framework Streamlit para a cria√ß√£o de uma interface interativa para o usu√°rio, permitindo a visualiza√ß√£o dos dados e a intera√ß√£o com as funcionalidades desenvolvidas.

Entre as funcionalidades desenvolvidas, est√£o a an√°lise de vendas por m√™s, a previs√£o de clientes para o pr√≥ximo m√™s, a an√°lise de dados de clientes cadastrados, a exibi√ß√£o de um mapa de localiza√ß√£o do estabelecimento e a cria√ß√£o de uma hist√≥ria do bar e restaurante.

Para utilizar o projeto, basta clonar o reposit√≥rio e instalar as depend√™ncias listadas no arquivo requirements.txt. Em seguida, execute o comando "streamlit run app.py" para iniciar a aplica√ß√£o.


## Status

| Topics  | Links                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Build   | [![Build Status](https://img.shields.io/travis/big-data-estacio/data/master)](https://travis-ci.com/github/big-data-estacio/data) [![codecov](https://codecov.io/gh/big-data-estacio/data/branch/master/graph/badge.svg)](https://codecov.io/gh/big-data-estacio/data) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) |
| Version | [![Version](https://img.shields.io/github/v/release/big-data-estacio/data)](https://github.com/big-data-estacio/data/releases) [![Release Notes](https://img.shields.io/badge/release-notes-blue)](https://github.com/big-data-estacio/data/releases)                                                                                                                                         |
| Docs    | [![Docs](https://img.shields.io/static/v1?label=docs&message=wiki&color=blue&logo=github)](https://github.com/big-data-estacio/data/wiki)                                                                                                                                                                                                                                                                                                           |
| Support | [![Gitter](https://badges.gitter.im/big-data-estacio/data.svg)](https://gitter.im/big-data-estacio/data?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![Mail](https://img.shields.io/badge/mail-support-brightgreen)](mailto:your-email@example.com)                                                                                                                                                     |
| Binder  | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/big-data-estacio/data/master?urlpath=lab)      |
| Technology | [![Flask](https://img.shields.io/badge/-Flask-black?style=flat&logo=flask)](https://flask.palletsprojects.com/) [![Hadoop](https://img.shields.io/badge/-Hadoop-black?style=flat&logo=hadoop)](https://hadoop.apache.org/) [![Kafka](https://img.shields.io/badge/-Kafka-black?style=flat&logo=apache-kafka)](https://kafka.apache.org/) [![PostgreSQL](https://img.shields.io/badge/-PostgreSQL-black?style=flat&logo=postgresql)](https://www.postgresql.org/) |
| Deploy                                                                                                                                                            | [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://)

---

1. Coleta de dados: O projeto utiliza diversas fontes de dados, como arquivos CSV, APIs e bancos de dados. Os dados s√£o coletados e processados em uma variedade de formatos para atender √†s necessidades do projeto. A coleta de dados √© um processo crucial para garantir que as informa√ß√µes corretas estejam dispon√≠veis para an√°lise e visualiza√ß√£o.

2. Processamento de dados: O projeto utiliza uma variedade de t√©cnicas para processar e limpar os dados coletados, incluindo o uso de bibliotecas Python para an√°lise e transforma√ß√£o de dados. Os dados s√£o organizados e limpos para garantir que estejam prontos para an√°lise e visualiza√ß√£o. O processamento de dados √© uma etapa importante no processo de garantir que as informa√ß√µes corretas estejam dispon√≠veis para o usu√°rio final.

3. Armazenamento de dados: O projeto utiliza uma variedade de tecnologias de armazenamento de dados, incluindo bancos de dados relacionais e n√£o relacionais, armazenamento em nuvem e arquivos CSV. Os dados s√£o armazenados de forma a garantir que estejam seguros e dispon√≠veis para an√°lise e visualiza√ß√£o. O armazenamento de dados √© um componente cr√≠tico do projeto, garantindo que os dados estejam dispon√≠veis quando necess√°rios.

4. An√°lise e visualiza√ß√£o de dados: O projeto utiliza ferramentas de an√°lise e visualiza√ß√£o de dados, como Power BI e bibliotecas Python, para extrair informa√ß√µes significativas dos dados coletados. As informa√ß√µes s√£o apresentadas de forma clara e intuitiva, permitindo que o usu√°rio final compreenda facilmente os insights obtidos a partir dos dados. A an√°lise e visualiza√ß√£o de dados s√£o etapas cr√≠ticas no processo de transforma√ß√£o de dados em informa√ß√µes √∫teis.

5. Atualiza√ß√£o e manuten√ß√£o: O projeto requer atualiza√ß√µes regulares para garantir que os dados estejam atualizados e precisos. Al√©m disso, a manuten√ß√£o do sistema √© importante para garantir que as tecnologias utilizadas estejam atualizadas e seguras. A atualiza√ß√£o e manuten√ß√£o do sistema s√£o etapas cr√≠ticas para garantir que o projeto continue funcionando de forma eficiente e eficaz ao longo do tempo.

O projeto utiliza conceitos de Big Data e Power BI. Vamos revisar como cada tecnologia √© aplicada no projeto:

1. Linguagem de programa√ß√£o Python para desenvolvimento do back-end e front-end da aplica√ß√£o web, utilizando o framework Streamlit para cria√ß√£o da interface do usu√°rio.
2. Pandas e Numpy para manipula√ß√£o e an√°lise de dados, realizando opera√ß√µes como sele√ß√£o, filtragem, agrupamento e jun√ß√£o de dados.
3. Matplotlib e Plotly para cria√ß√£o de gr√°ficos e visualiza√ß√µes de dados interativas.
4. Scikit-Learn para modelagem de dados, com algoritmos de aprendizado de m√°quina para previs√£o e classifica√ß√£o.
5. Power BI para cria√ß√£o de dashboards e relat√≥rios interativos e visuais a partir dos dados gerados e analisados na aplica√ß√£o web.

Combinando Big Data e Power BI, este projeto oferece uma solu√ß√£o completa para coletar, processar, analisar e visualizar grandes volumes de dados em tempo real, ajudando na tomada de decis√µes informadas e oferecendo insights valiosos para o neg√≥cio do restaurante "Pedacinho do C√©u".


## √çndice

1. [üöÄIntrodu√ß√£o](#introdu√ß√£o)
2. [üîÆArvore de Diretorios](#arvore-de-diretorios)
3. [üåÉArquitetura do projeto](#arquitetura-do-projeto)
4. [üéÜTecnologias Utilizadas](#tecnologias-utilizadas)
5. [üíéPr√©-requisitos](#pr√©-requisitos)
6. [‚ú®Instalando o Projeto](#instalando-o-projeto)
7. [üéâExecutando o Projeto](#executando-o-projeto)
   * [Configurando o ambiente virtual](#configurando-o-ambiente-virtual)
   * [Continuando a instala√ß√£o](#continuando-a-instala√ß√£o)
8. [üëæTestes](#testes)
9. [üî•Utilizando a API com Insomnia](#utilizando-a-api-com-insomnia)
10. [ü•∂Vers√£o atual](#vers√£o-atual)
11. [üëπColetando Dados](#coletando-dados)
12. [üëªProcessando Dados](#processando-dados)
13. [ü§ñVisualizando os Dados](#visualizando-os-dados)
14. [üëΩFuturas Atualiza√ß√µes](#futuras-atualiza√ß√µes)
15. [üê≥Tecnologias e conceitos utilizados](#tecnologias-e-conceitos-utilizados)
    * [Hadoop](#hadoop)
    * [Kafka](#kafka)
    * [Docker](#docker)
    * [SOLID](#solid)
    * [Padr√µes de commit](#padr√µes-de-commit)
16. [üß†Roadmap v1](#roadmap-v1)
    * [Travis CI](#travis-ci)
    * [Princ√≠pios SOLID](#princ√≠pios-solid)
    * [SQLAlchemy com PostgreSQL ou MySQL](#sqlalchemy-com-postgresql-ou-mysql)
    * [Autentica√ß√£o e autoriza√ß√£o](#autentica√ß√£o-e-autoriza√ß√£o)
    * [Interface de usu√°rio](#interface-de-usu√°rio)
17. [üß†Roadmap v2](#roadmap-v2)
    * [Roadmap v1.1](#roadmap-v1.1)
    * [Roadmap v1.2](#roadmap-v1.2)
    * [Roadmap v1.3](#roadmap-v1.3)
    * [Roadmap v2.0](#roadmap-v2.0)
18. [ü§ñO que √© o Apache Spark?](#o-que-√©-o-apache-spark?)
29. [üéñÔ∏èCrit√©rios de aceita√ß√£o do projeto](#crit√©rios-de-aceita√ß√£o-do-projeto)
20. [üëπContribuindo](#contribuindo)
21. [üëæContribuidores](#contribuidores)
22. [üéâLicen√ßa](#licen√ßa)


## Introdu√ß√£o

Este projeto √© um estudo de caso de Big Data e Power BI. O objetivo √© demonstrar como coletar, processar, analisar e visualizar grandes volumes de dados em tempo real usando ferramentas e tecnologias como Python, SQLite e Power BI.

O projeto √© baseado em um restaurante chamado "Pedacinho do C√©u". O restaurante est√° localizado em uma cidade tur√≠stica e serve comida tradicional da regi√£o. O restaurante est√° interessado em coletar dados de v√°rias fontes para analisar e obter insights sobre o neg√≥cio. O restaurante tamb√©m deseja criar visualiza√ß√µes e relat√≥rios interativos para ajudar na tomada de decis√µes e na compreens√£o de tend√™ncias e padr√µes nos dados.


## Arvore de Diretorios

Abaixo est√° a estrutura de diret√≥rios do projeto:

```bash
.
‚îú‚îÄ‚îÄ üìÇ .github
‚îú‚îÄ‚îÄ .husky
‚îú‚îÄ‚îÄ admin
‚îÇ   ‚îú‚îÄ‚îÄ conf
‚îÇ   ‚îú‚îÄ‚îÄ data_crawlers
‚îÇ   ‚îú‚îÄ‚îÄ target_url_crawlers
‚îÇ   ‚îú‚îÄ‚îÄ .gitignore
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ go_spider.py
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docs
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄapi
‚îÇ       ‚îú‚îÄ‚îÄ controller
‚îÇ       ‚îú‚îÄ‚îÄ model
‚îÇ       ‚îú‚îÄ‚îÄ routes
‚îÇ       ‚îî‚îÄ‚îÄ service
‚îÇ   ‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ error
‚îÇ   ‚îú‚îÄ‚îÄ log
‚îÇ   ‚îú‚îÄ‚îÄ public
‚îÇ   ‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ .editorconfig
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .npmrc
‚îú‚îÄ‚îÄ .travis.yml
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ debug.sh
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ Procfile
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ SECURITY.md
```


## Arquitetura do projeto

A arquitetura do projeto √© dividida em v√°rias partes:

* **`.github`**: diret√≥rio que cont√©m arquivos relacionados √† integra√ß√£o cont√≠nua com o GitHub.
* **`.husky`**: diret√≥rio que cont√©m arquivos relacionados √† configura√ß√£o do Husky, ferramenta que permite a execu√ß√£o de scripts no Git Hooks.
* **`admin`**: diret√≥rio que cont√©m arquivos relacionados √† administra√ß√£o do projeto, como scripts para extra√ß√£o de dados (data_crawlers) e URLs alvo (target_url_crawlers).
* **`docs`**: diret√≥rio que cont√©m arquivos relacionados √† documenta√ß√£o do projeto.
* **`src`**: diret√≥rio que cont√©m o c√≥digo fonte do projeto, organizado em diferentes subdiret√≥rios, como api (que cont√©m as rotas da aplica√ß√£o), data (que cont√©m os arquivos de dados), error (que cont√©m o tratamento de erros), log (que cont√©m os arquivos de logs) e public (que cont√©m arquivos est√°ticos, como imagens).
* **`app.py`**: arquivo que cont√©m a configura√ß√£o e inicializa√ß√£o da aplica√ß√£o Flask.
* **`docker-compose.yml`**: arquivo que cont√©m a configura√ß√£o do Docker Compose para a execu√ß√£o da aplica√ß√£o e do banco de dados.
* **`Dockerfile`**: arquivo que cont√©m a configura√ß√£o do Docker para a constru√ß√£o da imagem da aplica√ß√£o.
* **`Makefile`**: arquivo que cont√©m os comandos de automatiza√ß√£o de tarefas do projeto.
* **`README.md`**: arquivo que cont√©m a descri√ß√£o do projeto e sua documenta√ß√£o.
* **`requirements.txt`**: arquivo que cont√©m as depend√™ncias do projeto.
* **`LICENSE`**: arquivo que cont√©m as informa√ß√µes sobre a licen√ßa do projeto.


## Tecnologias Utilizadas

Neste projeto "Pedacinho do C√©u", diversas tecnologias s√£o utilizadas para coletar, processar, armazenar e visualizar dados. Abaixo est√° uma lista dessas tecnologias e como elas se encaixam no projeto:

1. **Docker Compose**: Ferramenta para definir e gerenciar aplica√ß√µes multi-container usando arquivos de configura√ß√£o (docker-compose.yml). √â usado para simplificar o processo de inicializa√ß√£o e gerenciamento de todos os servi√ßos envolvidos no projeto.

2. **SQLite3**: Sistema de gerenciamento de banco de dados relacional (RDBMS) utilizado para armazenar e gerenciar dados coletados e processados.

3. **Power BI**: Ferramenta de Business Intelligence (BI) da Microsoft para criar relat√≥rios e visualiza√ß√µes de dados. √â usado para analisar e visualizar os dados coletados e processados pelo projeto.

4. **Flask** (opcional): Microframework Python para desenvolvimento de aplica√ß√µes web. Pode ser usado para criar uma API RESTful que exp√µe os dados processados e armazenados para outras aplica√ß√µes ou servi√ßos.

5. **Pandas** (opcional): Biblioteca Python para manipula√ß√£o e an√°lise de dados. Pode ser usada em conjunto com o Apache Spark para realizar an√°lises e limpeza de dados em pequena escala antes de process√°-los no Spark.

6. **Python**: a linguagem de programa√ß√£o utilizada em todas as etapas do projeto, desde a coleta de dados at√© a an√°lise e visualiza√ß√£o. O Python √© uma linguagem de programa√ß√£o interpretada, orientada a objetos e de alto n√≠vel, que possui uma vasta biblioteca padr√£o e diversas bibliotecas de terceiros para processamento de dados.

7. **Streamlit**: uma biblioteca de c√≥digo aberto para cria√ß√£o de aplicativos web de dados em Python. O Streamlit √© utilizado no projeto para criar uma interface amig√°vel e interativa para visualiza√ß√£o dos dados.

8. **Plotly**: uma biblioteca de visualiza√ß√£o de dados interativa de c√≥digo aberto para Python. O Plotly √© utilizado no projeto para criar gr√°ficos e visualiza√ß√µes interativas a partir dos dados processados com o Pandas.

9. **Apache Airflow**: uma plataforma de orquestra√ß√£o de fluxo de trabalho para gerenciamento de tarefas de processamento de dados. O Apache Airflow √© utilizado no projeto para agendar e executar tarefas de coleta, processamento e an√°lise de dados de forma autom√°tica.


## Pr√©-requisitos

* Python 3.6+
* Apache Spark 3.0+
* Power BI Desktop
* Docker
* Docker Compose
* Node.js e npm
* Git
* Insomnia
* IDE de sua prefer√™ncia (PyCharm, VS Code, etc.)
* Terminal de sua prefer√™ncia (Git Bash, PowerShell, etc.)
* Sistema operacional Linux, macOS ou Windows
* Conhecimentos b√°sicos de Python e SQL


## Arquivo de configura√ß√£o package.json

O arquivo `package.json` cont√©m as depend√™ncias do projeto. Para instalar as depend√™ncias, execute o seguinte comando:

```bash
{
  "name": "restaurante",
  "version": "1.0.0",
  "description": "Projeto de restaurante utilizando Python e Flask",
  "main": "api/app.py",
  "scripts": {
    "docker-start": "docker-compose up --build -d",
    "docker-stop": "docker-compose down",
    "build": "docker build -t nome-da-imagem .",
    "lint": "flake8 src/ tests/",
    "start": "make",
    "test": "make test",
    "server": "streamlit run app.py",
    "dev": "nodemon server",
    "test-coverage": "make test-coverage",
    "pre-commit": "pre-commit install",
    "deploy": "./deploy.sh",
    "devops": "docker-compose -f docker-compose.devops.yml up --build -d",
    "devops-stop": "docker-compose -f docker-compose.devops.yml down"
  },
  "keywords": [
    "restaurante",
    "python",
    "flask"
  ],
  "author": "grupo-estacio",
  "license": "MIT",
  "devDependencies": {
    "nodemon": "^2.0.22"
  },
  "dependencies": {
    "Flask": "^2.0.0",
    "Flask-Cors": "^3.0.10",
    "Flask-JWT-Extended": "^4.2.3",
    "Flask-Mail": "^0.9.1",
    "Flask-RESTful": "^0.3.9",
    "Flask-SQLAlchemy": "^3.0.0",
    "PyMySQL": "^1.0.2"
  }
}
```


## Instalando o Projeto

1. Clone o reposit√≥rio:

```bash
git clone https://github.com/big-data-estacio/data.git
cd pedacinho_do_ceu
```


## Executando o Projeto

### Configurando o ambiente virtual

√â recomendado utilizar um ambiente virtual para isolar as depend√™ncias do projeto. Siga os passos abaixo para configurar e ativar o ambiente virtual usando o `venv`:

1. Instale o m√≥dulo `venv`, caso ainda n√£o tenha, com o seguinte comando:

```bash
python3 -m pip install --user virtualenv
```

2. Navegue at√© a pasta do projeto e crie um ambiente virtual:

```bash
python3 -m venv venv

# ou, se voc√™ estiver no Windows

py -m venv venv
```

3. Ative o ambiente virtual:

* No Windows:

```bash
.\venv\Scripts\activate

# ou, se voc√™ estiver usando o Git Bash

source venv/Scripts/activate
```

* No macOS e Linux:

```bash
source venv/bin/activate
```

4. Ap√≥s a ativa√ß√£o do ambiente virtual, seu terminal deve mostrar o prefixo `(venv)`.

Agora voc√™ pode executar o projeto com as depend√™ncias instaladas no ambiente virtual. Lembre-se de ativar o ambiente virtual sempre que for trabalhar no projeto.

5. Instale todas as bibliotecas necess√°rias usando o comando `pip install`:

```bash
pip install biblioteca1 biblioteca2 biblioteca3
```

Substitua `biblioteca1`, `biblioteca2` e `biblioteca3` pelos nomes das bibliotecas que voc√™ deseja instalar.

6. Agora que todas as bibliotecas est√£o instaladas, execute o seguinte comando para gerar o arquivo `requirements.txt`:

```bash
pip freeze > requirements.txt
```

O comando `pip freeze` listar√° todas as bibliotecas instaladas no ambiente virtual e suas vers√µes espec√≠ficas. O operador `>` redirecionar√° a sa√≠da para o arquivo `requirements.txt`, criando-o ou atualizando-o, se j√° existir.

7. Instale as depend√™ncias do projeto utilizando o arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

> arquivo requirements.txt para esse projeto com as depend√™ncias necess√°rias.

```bash
altair==4.1.0
appdirs==1.4.4
astor==0.8.1
attrs==19.3.0
backcall==0.1.0
base58==2.0.0
bleach==3.1.5
blinker==1.4
boto3==1.13.24
botocore==1.16.24
cachetools==4.1.0
certifi==2020.4.5.1
chardet==3.0.4
click==7.1.2
colorama==0.4.3
cycler==0.10.0
decorator==4.4.2
defusedxml==0.6.0
distlib==0.3.0
docopt==0.6.2
docutils==0.15.2
entrypoints==0.3
enum-compat==0.0.3
jedi==0.17.0
Jinja2==2.11.2
jmespath==0.10.0
jsonschema==3.2.0
kiwisolver==1.2.0
MarkupSafe==1.1.1
matplotlib==3.2.1
mistune==0.8.4
nbconvert==5.6.1
nbformat==5.0.6
numpy==1.18.5
packaging==20.4
pandas==1.0.4
pandocfilters==1.4.2
parso==0.7.0
pathtools==0.1.2
pickleshare==0.7.5
Pillow==7.1.2
pipreqs==0.4.10
plotly==4.14.3
plotting==0.0.6
pydeck==0.4.0b1
Pygments==2.6.1
pywinpty==0.5.7
pyzmq==19.0.1
qtconsole==4.7.4
QtPy==1.9.0
requests==2.23.0
retrying==1.3.3
s3transfer==0.3.3
scipy==1.4.1
seaborn==0.10.1
Send2Trash==1.5.0
six==1.15.0
streamlit==0.61.0
terminado==0.8.3
testpath==0.4.4
toml==0.10.1
toolz==0.10.0
tornado==5.1.1
traitlets==4.3.3
tzlocal==2.1
urllib3==1.25.9
validators==0.15.0
virtualenv==20.0.21
watchdog==0.10.2
wcwidth==0.2.3
webencodings==0.5.1
widgetsnbextension==3.5.1
yarg==0.1.9
```

8. Quando terminar de trabalhar no projeto, voc√™ pode desativar o ambiente virtual com o seguinte comando:

```bash
deactivate
```

### üì¶ Continuando a instala√ß√£o

1. Crie um arquivo `.env` na raiz do projeto com as vari√°veis de ambiente necess√°rias. Voc√™ pode usar o arquivo `.env.example` como modelo.

2. Instale as depend√™ncias do projeto com o yarn:

```bash
yarn add
```

### Para iniciar o projeto "Pedacinho do C√©u" com o Docker, siga estas etapas:

1. Certifique-se de que o Docker e o Docker Compose estejam instalados em seu sistema. Se voc√™ ainda n√£o os instalou, siga as instru√ß√µes de instala√ß√£o no site oficial do Docker: `https://docs.docker.com/get-docker/` e `https://docs.docker.com/compose/install/`

2. Abra um terminal e navegue at√© o diret√≥rio raiz do projeto `pedacinho_do_ceu`.

3. Execute o seguinte comando para criar e iniciar os containers do projeto, conforme definido no arquivo `docker-compose.yml`:

- ```docker build -t streamlit-ts-ml:0.1.0 -f Dockerfile .```
- ```docker run -p 8501:8501 streamlit-ts-ml:0.1.0```
- Open `http://localhost:8501/`
<!-- - Build with ```docker build -t ts-forecast-app .``` (takes some time!)
- Run with ```docker run -p 8501:8501 ts-forecast-app:latest``` -->

Isso iniciar√° os servi√ßos do Docker, a API, como o SQLite3 e o servi√ßo de processamento de dados se necess√°rio.

4. Abra os relat√≥rios do Power BI na pasta `data_visualization/power_bi_reports/` para visualizar e analisar os dados processados. Se voc√™ n√£o possui o Power BI Desktop, fa√ßa o download e instale-o a partir do site oficial: `https://powerbi.microsoft.com/en-us/desktop/`

5. Ao longo do desenvolvimento do projeto, voc√™ pode modificar e ajustar os arquivos na pasta `src/` conforme necess√°rio para aprimorar a an√°lise e o processamento dos dados.

Lembre-se de que, dependendo da configura√ß√£o do projeto, algumas etapas podem variar ou exigir a√ß√µes adicionais. Ajuste as etapas conforme necess√°rio para se adequar √†s necessidades espec√≠ficas do seu projeto.

6. Quando terminar de usar o projeto, pare os servi√ßos do Docker Compose pressionando `Ctrl+C` no terminal onde o Docker Compose est√° sendo executado. Para remover os cont√™ineres e os volumes, execute:

```bash
docker-compose down --remove-orphans --volumes
```

### Para iniciar o projeto "Pedacinho do C√©u" sem o Docker, siga estas etapas:

1. Abra um terminal e navegue at√© o diret√≥rio raiz do projeto `data`.

2. Execute o seguinte comando para executar o projeto:

```bash
yarn run server

# ou

cd [directory]
streamlit run app.py --server.address 0.0.0.0 --server.port [your port]
# http://0.0.0.0:[your port]
```

### Iniciando a API

1. Inicie o servidor da API:
  
```bash
npm start
```

> J√° o arquivo api/app.py, √© o arquivo principal da API, onde √© feita a conex√£o com o banco de dados e a defini√ß√£o dos endpoints.

Ele √© executado na porta 5000, e pode ser acessado em `http://localhost:5000/`

## Utilizando a API

A API RESTful permite gerenciar os dados armazenados no banco de dados PostgreSQL. Para utilizar a API, voc√™ pode fazer requisi√ß√µes HTTP para os seguintes endpoints:

Nesse arquivo, se encontram os endpoints da API, que s√£o:

* **`GET /`**: Lista todos os itens armazenados no banco de dados.
* **`GET /<id>`**: Retorna os detalhes de um item espec√≠fico com base no ID.
* **`POST /`**: Adiciona um novo item ao banco de dados.
* **`PUT /<id>`**: Atualiza os detalhes de um item espec√≠fico com base no ID.
* **`DELETE /<id>`**: Remove um item espec√≠fico com base no ID.

> @TODO: Adicionar mais detalhes sobre a API
> @Note: A API foi desenvolvida com o framework FastAPI, que √© um framework web ass√≠ncrono de alto desempenho, f√°cil de aprender, r√°pido para codificar, pronto para produ√ß√£o. Ele √© constru√≠do com base no Starlette e Pydantic.

o arquivo `api/app.py` est√° configurado com os scripts de coleta de dados e processamento de dados, para que os dados sejam coletados e processados automaticamente quando a API for iniciada:

```python
from flask import Flask
from routes.bebidas import bebidas_bp
from routes.clientes import clientes_bp

app = Flask(__name__)
app.register_blueprint(bebidas_bp)
app.register_blueprint(clientes_bp)

if __name__ == '__main__':
    app.run(debug=True)
```

A API estar√° dispon√≠vel na porta 5000.

* Abra o Power BI Desktop e carregue os relat√≥rios na pasta `data_visualization/power_bi_reports`.


## Testes

Execute os testes unit√°rios e de integra√ß√£o usando o seguinte comando:

```bash
python -m unittest discover tests
```


## Debugging e Logging

Para depurar o projeto, voc√™ pode usar o depurador interativo do Python. Para isso, basta adicionar o seguinte c√≥digo ao arquivo que voc√™ deseja depurar:

```python
import pdb; pdb.set_trace()
```

Em seguida, execute o arquivo normalmente e o depurador interativo ser√° iniciado quando o c√≥digo atingir o ponto de interrup√ß√£o.

Para adicionar logs ao projeto, voc√™ pode usar o m√≥dulo `logging` do Python. Para isso, basta adicionar o seguinte c√≥digo ao arquivo que voc√™ deseja depurar:

```python
import logging

logging.basicConfig(level=logging.DEBUG)
```

> como que fica o arquivo de log logs/app.log

```log
2023-04-25 00:31:31,719 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:33:49,546 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:34:44,584 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 00:34:46,857 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 00:34:46,859 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 00:34:46,860 - INFO - Probing node bootstrap-0 broker version
2023-04-25 00:34:47,912 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 00:34:47,915 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 00:36:12,026 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:37:32,931 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:38:27,387 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:38:29,541 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 09:38:29,542 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 09:38:29,542 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:38:30,582 - WARNING - DNS lookup failed for kafka:9092, exception was [Errno -2] Name or service not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?
2023-04-25 09:38:30,582 - ERROR - DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)
2023-04-25 09:39:43,938 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:39:44,780 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:39:44,780 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:39:44,781 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:39:44,782 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:39:44,827 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:39:44,828 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:39:44,828 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:41:52,885 - INFO - A aplica√ß√£o foi encerrada com sucesso.
2023-04-25 09:41:53,647 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:41:53,648 - INFO - Probing node bootstrap-0 broker version
2023-04-25 09:41:53,649 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:41:53,649 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:41:53,693 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-04-25 09:41:53,694 - ERROR - Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.
2023-04-25 09:41:53,694 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED
2023-04-25 09:42:12,859 - INFO - A aplica√ß√£o foi encerrada com sucesso.
```

> e o arquivo de debug:

[![debug](docs/debug.png)](docs/debug.png)


## Utilizando a API com Insomnia

Para testar e interagir com a API, voc√™ pode usar o Insomnia REST Client, que √© uma aplica√ß√£o de desktop para gerenciar requisi√ß√µes HTTP e API.

1. Instale o Insomnia em seu computador: Insomnia Download
2. Abra o Insomnia e crie um novo workspace ou use um existente.
3. Para adicionar as rotas da API ao seu workspace, clique em `Create Request`. Escolha o m√©todo HTTP (GET, POST, PUT, DELETE) e insira a URL da API (por exemplo, `http://localhost:5000/` para listar todos os itens). Adicione os par√¢metros, cabe√ßalhos ou corpo da requisi√ß√£o conforme necess√°rio e clique em `Send` para executar a requisi√ß√£o.
4. Voc√™ pode criar v√°rias requisi√ß√µes para diferentes endpoints da API e organiz√°-las em pastas para facilitar a navega√ß√£o.

Agora voc√™ pode usar o Insomnia para testar e explorar a API RESTful do projeto Pedacinho do C√©u.


## Vers√£o atual
A vers√£o atual do projeto inclui as seguintes funcionalidades:

* Coleta e armazenamento de dados em tempo real com Hadoop e Kafka.
* API RESTful para interagir com os dados e servi√ßos.
* Docker Compose para facilitar a instala√ß√£o e configura√ß√£o do projeto.


## Coletando Dados

Os dados podem ser coletados usando o script `data_collection/data_collector.py`. Para coletar os dados, voc√™ pode executar o seguinte comando:

```bash
python data_collection/data_collector.py
```

Os dados coletados ser√£o armazenados em arquivos no diret√≥rio `data_collection/data`.


## Processando Dados

Os dados podem ser processados usando o script `data_processing/data_processor.py`. Para processar os dados, voc√™ pode executar o seguinte comando:

```bash
python data_processing/data_processor.py
```

Os dados processados ser√£o armazenados no banco de dados PostgreSQL.


## Visualizando os Dados

Os dados podem ser visualizados usando relat√≥rios do Power BI. Para visualizar os relat√≥rios, voc√™ pode fazer o download do Power BI Desktop [aqui](https://powerbi.microsoft.com/pt-br/desktop/). Ap√≥s instalar o Power BI Desktop, voc√™ pode abrir o arquivo `pedacinho_do_ceu.pbix` para visualizar os relat√≥rios.


## Futuras Atualiza√ß√µes

* Melhoria na coleta de dados
* Implementa√ß√£o de novas an√°lises e visualiza√ß√µes
* Adi√ß√£o de mais testes
* Melhoria na documenta√ß√£o

O projeto "Pedacinho do C√©u" est√° em constante evolu√ß√£o e melhorias. Abaixo est√£o algumas das atualiza√ß√µes futuras planejadas para o projeto:

1. **Expans√£o das fontes de dados**: Atualmente, o projeto utiliza um conjunto limitado de fontes de dados. No futuro, pretendemos expandir a lista de fontes de dados para enriquecer ainda mais nossas an√°lises e visualiza√ß√µes.

2. **Otimiza√ß√£o do processamento de Big Data**: Continuaremos a melhorar a efici√™ncia e a velocidade do processamento de Big Data usando t√©cnicas de otimiza√ß√£o avan√ßadas e atualiza√ß√µes nas ferramentas e tecnologias utilizadas.

3. **Visualiza√ß√µes interativas**: Atualmente, o projeto utiliza relat√≥rios est√°ticos do Power BI. Planejamos adicionar visualiza√ß√µes interativas e em tempo real para tornar os dados mais acess√≠veis e f√°ceis de entender.

4. **Machine Learning**: Exploraremos a implementa√ß√£o de algoritmos de aprendizado de m√°quina para identificar padr√µes e tend√™ncias nos dados e fornecer insights adicionais.

5. **Integra√ß√£o com outras plataformas**: Planejamos adicionar suporte para integra√ß√£o com outras plataformas de visualiza√ß√£o de dados e an√°lise, como Tableau e Looker.

6. **APIs e microservi√ßos**: Atualmente, o projeto possui uma API simples para acessar os dados. No futuro, pretendemos expandir a API e usar microservi√ßos para fornecer uma solu√ß√£o mais modular e escalon√°vel.

7. **Seguran√ßa e privacidade**: Continuaremos a aprimorar a seguran√ßa e a privacidade dos dados, garantindo a conformidade com os regulamentos de prote√ß√£o de dados e as melhores pr√°ticas do setor.

8. **Documenta√ß√£o e tutoriais**: A documenta√ß√£o e os tutoriais ser√£o atualizados regularmente para garantir que sejam claros, abrangentes e atualizados com as √∫ltimas informa√ß√µes do projeto.


## Tecnologias e conceitos utilizados

### Docker
O projeto √© executado em cont√™ineres Docker para facilitar a implanta√ß√£o e a escalabilidade.

### SOLID
Os princ√≠pios SOLID foram aplicados neste projeto para garantir um c√≥digo limpo, modular e f√°cil de manter.

## Padr√µes de commit
Ao fazer um commit, siga as diretrizes de mensagens de commit personalizadas e com emojis, utilizando at√© 200 caracteres.


## Roadmap v1

* Adicionar suporte ao Travis CI para integra√ß√£o cont√≠nua.
* Aplicar os princ√≠pios SOLID ao projeto para melhorar a manutenibilidade e escalabilidade.
* Atualizar o SQLAlchemy para usar um banco de dados mais robusto, como PostgreSQL ou MySQL, em vez de SQLite.
* Implementar autentica√ß√£o e autoriza√ß√£o para proteger as rotas da API.
* Adicionar testes automatizados para garantir a qualidade do c√≥digo.
* Implementar uma interface de usu√°rio para interagir com a API.

### Travis CI

1. Para adicionar suporte ao Travis CI, siga estas etapas:
2. V√° para `travis-ci.com` e fa√ßa login com sua conta do GitHub.
3. Ative o reposit√≥rio do projeto nas configura√ß√µes do Travis CI.
4. Crie um arquivo `.travis.yml` na raiz do projeto com o seguinte conte√∫do:

```yml
language: python
python:
  - "3.8"

install:
  - pip install -r api/requirements.txt

script:
  - pytest
```

5. Vincule seu reposit√≥rio do GitHub ao Travis CI e habilite a integra√ß√£o cont√≠nua para o projeto.


### Princ√≠pios SOLID
Para aplicar os princ√≠pios SOLID ao projeto, siga estas diretrizes:

1. Single Responsibility Principle (SRP): Separe as responsabilidades do c√≥digo em classes e fun√ß√µes distintas. Por exemplo, separe a l√≥gica de neg√≥cio da l√≥gica de banco de dados.

2. Open/Closed Principle (OCP): Crie classes e fun√ß√µes extens√≠veis sem modificar seu c√≥digo-fonte. Por exemplo, use a heran√ßa e a composi√ß√£o para estender a funcionalidade de uma classe.

3. Liskov Substitution Principle (LSP): As subclasses devem ser substitu√≠veis por suas classes base sem alterar a corre√ß√£o do programa. Por exemplo, verifique se as subclasses seguem o contrato estabelecido pelas classes base.

4. Interface Segregation Principle (ISP): Divida as interfaces grandes em interfaces menores e mais espec√≠ficas. Por exemplo, crie interfaces separadas para opera√ß√µes de leitura e grava√ß√£o em vez de uma interface √∫nica.

5. Dependency Inversion Principle (DIP): Depend√™ncias de alto n√≠vel n√£o devem depender de depend√™ncias de baixo n√≠vel; em vez disso, ambas devem depender de abstra√ß√µes. Por exemplo, use a inje√ß√£o de depend√™ncia para inverter as depend√™ncias entre os m√≥dulos.

### SQLAlchemy com PostgreSQL ou MySQL
Para usar o PostgreSQL ou MySQL com SQLAlchemy, siga estas etapas:

1. Instale os pacotes necess√°rios:

```bash
pip install psycopg2 sqlalchemy psycopg2-binary
```

OU

```bash
pip install pymysql sqlalchemy
```

2. Atualize a string de conex√£o do banco de dados no arquivo api/database.py para usar PostgreSQL ou MySQL:

Para PostgreSQL:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "postgresql://user:password@localhost/dbname"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

Para MySQL:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

DATABASE_URL = "mysql+pymysql://user:password@localhost/dbname"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

3. Atualize as configura√ß√µes do banco de dados em `api/__init__.py` para usar a nova string de conex√£o.

### Autentica√ß√£o e autoriza√ß√£o
Para implementar a autentica√ß√£o e autoriza√ß√£o, voc√™ pode usar a extens√£o Flask-Login ou Flask-Security. Siga a documenta√ß√£o oficial de cada extens√£o para configurar a autentica√ß√£o e autoriza√ß√£o no projeto.

### Testes automatizados
Para adicionar testes automatizados, siga estas etapas:

1. Instale o pacote pytest:
  
```bash
pip install pytest
```

2. Crie um diret√≥rio `tests` na raiz do projeto.

3. Dentro do diret√≥rio `tests`, crie arquivos de teste para cada m√≥dulo do projeto. Por exemplo, voc√™ pode criar um arquivo `test_restaurant_service.py` para testar a classe RestaurantService.

4. Escreva testes usando o pytest para cada fun√ß√£o e classe do projeto.

5. Execute os testes usando o comando `pytest` no terminal.

### Interface de usu√°rio
Para implementar uma interface de usu√°rio, voc√™ pode criar uma aplica√ß√£o web usando HTML, CSS e JavaScript para interagir com a API. Voc√™ pode usar bibliotecas como React, Angular ou Vue.js para criar a interface do usu√°rio.


## Roadmap v2

### Vers√£o 1.1
* Implementar autentica√ß√£o e autoriza√ß√£o na API.
* Adicionar suporte para diferentes formatos de dados (JSON, XML, etc.).
* Desenvolver uma interface gr√°fica para visualizar os dados e interagir com a API.

### Vers√£o 1.2
* Adicionar suporte para processamento de dados em tempo real usando Apache Spark ou Flink.
* Implementar algoritmos de machine learning para an√°lise de dados e gera√ß√£o de insights.
* Integrar com outras fontes de dados e APIs de terceiros.

### Vers√£o 1.3
* Adicionar suporte para escalabilidade horizontal e balanceamento de carga.
* Implementar monitoramento e alertas para garantir a sa√∫de e o desempenho do sistema.
* Desenvolver testes de carga e estresse para garantir a estabilidade do projeto em ambientes de produ√ß√£o.

### Vers√£o 2.0
* Redesenhar a arquitetura do projeto para melhorar a modularidade e a manutenibilidade.
* Adicionar suporte para outras linguagens de programa√ß√£o e frameworks.
* Implementar uma API GraphQL para maior flexibilidade na consulta e manipula√ß√£o de dados.


## O que √© o Apache Spark?

![](docs/spark-streaming-datanami.png)
# Apache_Spark
:sparkles::tada: Este reposit√≥rio explica sobre o Apache Spark com pr√°ticas :tada::sparkles:

- √â uma ferramenta de processamento em tempo real desenvolvida para resolver o problema de trabalhar com dados em tempo real.
- √â tratado pelo Apache.
- √â muito r√°pido, d√° resultado no clique.
- Usa avalia√ß√£o pregui√ßosa, ou seja, processa sempre que necess√°rio.
- Como o MapReduce n√£o conseguiu lidar com dados em tempo real, o Spark entrou em cena para ajudar.
- Agora √© usado por muitos grandes gigantes da tecnologia como Oracle, Amazon, Microsoft, Visa, Cisco, Verizon, Hortonworks.
- Como acima, temos 3.000 empresas usando o Apache Spark.

<div align="center">
  <a href="https://www.youtube.com/watch?v=4TE6AGQ0IzI" target="_blank"
  alt="IMAGE ALT TEXT" width="20" height="10" border="10"><img src="https://i.ytimg.com/vi/4TE6AGQ0IzI/maxresdefault.jpg"></a>
</div>

## Baixando o Apache Spark

- Para baixar o Apache Spark, acesse: [Download Apache Spark](https://spark.apache.org/downloads.html)

### Instalando o Apache Spark
- Para Mac: [Instala√ß√£o do Apache Spark](https://github.com/HarshitDawar55/Apache_Spark/blob/master/docs/Installation_Of_Spark_In_Mac.docx)

### Linguagens usadas para programa√ß√£o no Spark:

- Python
- Escala

### Instru√ß√µes para executar scripts do Apache Spark

- Copie o c√≥digo do respectivo script e cole-o no shell Spark correspondente para executar o c√≥digo.

### Instru√ß√µes para executar scripts python
- Abra o terminal (no ambiente Linux)/Command Propmpt (Windows).
- Execute "python <nome do script>.py"


## Crit√©rios de aceita√ß√£o do projeto

Com base nas informa√ß√µes fornecidas at√© agora, o projeto est√° usando as seguintes tecnologias e t√©cnicas:

1. **`Apache Hadoop e Apache Spark`** como ferramentas de Big Data
2. **`Integra√ß√£o com Python`** para executar scripts e processar dados usando o Spark
3. **`Flask`** para criar uma API RESTful

De acordo com os crit√©rios do professor, esse projeto estaria atendendo aos seguintes pontos:

* Usando alguma ferramenta de Big Data com Python (+++): O projeto est√° usando o Apache Hadoop e o Apache Spark, que s√£o ferramentas de Big Data, e est√° integrando-as com Python para executar scripts e processar dados.

> Como resultado, o projeto deve passar na mat√©ria com uma nota alta, pois est√° utilizando ferramentas de Big Data e integrando-as com Python, o que √© o crit√©rio de maior pontua√ß√£o (+++).

Al√©m disso, o projeto tamb√©m utiliza outras tecnologias e pr√°ticas relevantes, como o Flask para criar uma API RESTful e o SQLAlchemy como um ORM para lidar com as opera√ß√µes de banco de dados. Esses aspectos podem n√£o ser mencionados diretamente nos crit√©rios de avalia√ß√£o do professor, mas certamente agregam valor ao projeto e demonstram a capacidade de trabalhar com v√°rias tecnologias e bibliotecas em Python.

Em resumo, o projeto atende ao crit√©rio de maior pontua√ß√£o, utilizando ferramentas de Big Data e integrando-as com Python. Isso deve resultar em uma avalia√ß√£o positiva para a mat√©ria.

## Contribuindo

Para contribuir com este projeto, siga estas etapas:

1. Fa√ßa um fork do reposit√≥rio.
2. Crie um branch com suas altera√ß√µes (`git checkout -b my-feature`).
3. Fa√ßa commit das suas altera√ß√µes (`git commit -m 'Add some feature'`).
4. Fa√ßa push para o branch (`git push origin my-feature`).
5. Crie uma nova Pull Request.


## Contribuidores

Agradecemos √†s seguintes pessoas que contribu√≠ram para este projeto:

<a href="https://github.com/gutorafael">
  <img src="https://contrib.rocks/image?repo=gutorafael/gutorafael" />
</a>
<a href="https://github.com/gfucci">
  <img src="https://contrib.rocks/image?repo=gfucci/desafio_acaodireta" />
</a>
<a href="https://github.com/lele-sf">
  <img src="https://contrib.rocks/image?repo=lele-sf/lele-sf" />
</a>
<a href="https://github.com/estevam5s">
  <img src="https://contrib.rocks/image?repo=estevam5s/rest-api-nasa" />
</a>
<a href="https://github.com/PedroDellaMea">
  <img src="https://contrib.rocks/image?repo=PedroDellaMea/Atividades-HTML-AV1" />
</a>
<a href="https://github.com/AldairSouto">
  <img src="https://contrib.rocks/image?repo=AldairSouto/AldairSouto" />
</a>


## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - consulte o arquivo `LICENSE` para obter detalhes.